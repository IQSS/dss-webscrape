[
["index.html", "Template Introduction Table of Contents Authors and Sources", " Template May 2020 Introduction Table of Contents Here, we outline how the guide is organized into parts. First, we… Second, we… Lastly, we… Here we provide an outside link to important content which puts some useful information for this tutorial/workshop at our fingertips. Here we specify where people can provide feedback! Please email help@iq.harvard.edu Authors and Sources Here we acknowledge a few people who helped make this tutorial/workshop possible. We also reference any sources that material was taken from. "],
["introduction-1.html", "1 Introduction 1.1 How does the web work? 1.2 Uniform Resource Locator (URL) 1.3 Document Object Model (DOM) 1.4 Decision Tree for Choosing Web Scraping Approaches", " 1 Introduction We start with the basic concepts that are generic to all web scraping approaches. 1.1 How does the web work? 1.1.1 Components Computers connected to the web are called clients and servers. A simplified diagram of how they interact might look like this: Clients are the typical web user’s internet-connected devices (for example, your computer connected to your Wi-Fi) and web-accessing software available on those devices (usually a web browser like Firefox or Chrome). Servers are computers that store webpages, sites, or apps. When a client device wants to access a webpage, a copy of the webpage is downloaded from the server onto the client machine to be displayed in the user’s web browser. HTTP is a language for clients and servers to speak to each other. 1.1.2 So what happens? When you type a web address into your browser: The browser finds the address of the server that the website lives on. The browser sends an HTTP request message to the server, asking it to send a copy of the website to the client. If the server approves the client’s request, the server sends the client a 200 OK message, and then starts displaying the website in the browser. 1.2 Uniform Resource Locator (URL) To retrieve information from the website (i.e., make a request), we need to know the location of the information we want to collect. The Uniform Resource Locator (URL) — commonly know as a “web address”, specifies the location of a resource (such as a web page) on the internet. A URL is usually composed of 5 parts: The 4th part, the “query string”, contains one or more parameters. The 5th part, the “fragment”, is an internal page reference and may not be present. For example, the URL we want to retrieve data from has the following structure: protocol domain path parameters https www.harvardartmuseums.org browse load_amount=10&amp;offset=0 It is often convenient to create variables containing the domain(s) and path(s) you’ll be working with, as this allows you to swap out paths and parameters as needed. Note that the path is separated from the domain with / and the parameters are separated from the path with ?. If there are multiple parameters they are separated from each other with a &amp;. 1.3 Document Object Model (DOM) To parse HTML, we need to have a nice tree structure that contains the whole HTML file through which we can locate the information. This tree-like structure is the Document Object Model (DOM). DOM is a cross-platform and language-independent interface that treats an XML or HTML document as a tree structure wherein each node is an object representing a part of the document. The DOM represents a document with a logical tree. Each branch of the tree ends in a node, and each node contains objects. DOM methods allow programmatic access to the tree; with them one can change the structure, style or content of a document. The following is an example of DOM hierarchy in an HTML document: 1.4 Decision Tree for Choosing Web Scraping Approaches There are many commonly used web scraping approaches. The decision tree as follows will help you to decide upon the best approach to use for a particular web site. "],
["approaches-to-web-scraping.html", "2 Approaches to Web Scraping 2.1 Approaches to Scraping a Static Web Page 2.2 Approaches to Scraping a Dynamic Web Page", " 2 Approaches to Web Scraping If the content you are viewing in your browser does not match the content you see in the HTML source code you are retrieving from the site, then you are experiencing the dynamic websites. Otherwise, if they match with each other, the websites are static. The mismatch is due to the execution of JavaScript that changes the HTML elements on the page. You could view the original HTML via View page source. You could view the revised HTML in your browser if it executes JavaScript in the Elements window via Inspecting the web page. 2.1 Approaches to Scraping a Static Web Page There are three approaches to extracting data from a static webpage that has been downloaded: using regular expressions, using Beautiful Soup module, and finally using lxml module. We use this static student profile webpage to provide examples for each approach. Suppose that we want to scrape the student name. The data we are interested in is found in this part of the HTML. The student name is included within a &lt;td&gt; element of class w2p_fw, which is the child of a &lt;tr&gt; element of ID students_name_row. &lt;table&gt; &lt;tr id=&quot;students_name_row&quot;&gt;&lt;td class=&quot;w2p_fl&quot;&gt;&lt;label for=&quot;students_name&quot; id=&quot;students_name_label&quot;&gt;Name:&lt;/label&gt;&lt;/td&gt;&lt;td class=&quot;w2p_fw&quot;&gt;Adams&lt;/td&gt; &lt;td class=&quot;w2p_fc&quot;&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr id=&quot;students_school_row&quot;&gt;&lt;td class=&quot;w2p_fl&quot;&gt;&lt;label for=&quot;students_school&quot; id=&quot;students_school_label&quot;&gt;School:&lt;/label&gt;&lt;/td&gt;&lt;td class=&quot;w2p_fw&quot;&gt;IV&lt;/td&gt; &lt;td class=&quot;w2p_fc&quot;&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr id=&quot;students_level_row&quot;&gt;&lt;td class=&quot;w2p_fl&quot;&gt;&lt;label for=&quot;students_level&quot; id=&quot;students_level_label&quot;&gt;Advanced:&lt;/label&gt;&lt;/td&gt;&lt;td class=&quot;w2p_fw&quot;&gt;No&lt;/td&gt; &lt;td class=&quot;w2p_fc&quot;&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; 2.1.1 Regular Expressions Regular expressions directly work on a downloaded web page with no need of parsing it into a certain format and try to match the content of the part of the HTML that contains the data you want to scrape from. There is a thorough overview of regular expressions here. In this example, we need to match the &lt;td class=\"w2p_fw\"&gt; tag to scrape the student name. But this tag is used for multiple student profile attributes. To isolate the name, we select the first element, as shown in the code below. import re import requests url = &#39;https://iqssdss2020.pythonanywhere.com/tutorial/static/views/Adams.html&#39; html = requests.get(url) mylist = re.findall(&#39;&lt;td class=&quot;w2p_fw&quot;&gt;(.*?)&lt;/td&gt;&#39;, html.text) print(mylist) ## [&#39;Adams&#39;, &#39;IV&#39;, &#39;No&#39;] name = re.findall(&#39;&lt;td class=&quot;w2p_fw&quot;&gt;(.*?)&lt;/td&gt;&#39;, html.text)[0] print(name) ## Adams This solution works now but could easily fail if the web page is updated later. Consider if the student ID data is inserted right before the student name. Then we must change the code to select the second element. The general solution to make a regular expression scraper more robust is to include the parent element, which has an ID, so it ought to be unique: import re import requests url = &#39;https://iqssdss2020.pythonanywhere.com/tutorial/static/views/Adams.html&#39; html = requests.get(url) mylist = re.findall(&#39;&lt;tr id=&quot;students_name_row&quot;&gt;&lt;td class=&quot;w2p_fl&quot;&gt;&lt;label for=&quot;students_name&quot; id=&quot;students_name_label&quot;&gt;Name:\\ &lt;/label&gt;&lt;/td&gt;&lt;td class=&quot;w2p_fw&quot;&gt;(.*?)&lt;/td&gt;&#39;, html.text) print(mylist) ## [&#39;Adams&#39;] This solution is better. However, there are many other ways the web page could be updated that still break the regular expression. For example, double quotation might be changed to single for class name, extra space could be added between the &lt;td&gt; tags, or the name_label could be changed. The general solution to it is to make the regular expression as generic as possible to support various possibilities: import re import requests url = &#39;https://iqssdss2020.pythonanywhere.com/tutorial/static/views/Adams.html&#39; html = requests.get(url) mylist = re.findall(&#39;&lt;tr id=&quot;students_name_row&quot;&gt;.*?&lt;td\\s*class=[&quot;\\&#39;]w2p_fw[&quot;\\&#39;]&gt;(.*?)&lt;/td&gt;&#39;, html.text) print(mylist) ## [&#39;Adams&#39;] This regular expression is more robust to webpage updates but is more difficult to construct, becoming even unreadable. But still, there are other minor layout changes that would break it, such as if a title attribute is added to the &lt;td&gt; tag. From this example, regular expressions provide a quick way to scrape data without the step of parsing but are too brittle and will easily break when a web page is updated. 2.1.2 Beautiful Soup Beautiful Soup is a popular module that parses a downloaded web page into a certain format and then provides a convenient interface to navigate content. The official documentation of Beautiful Soup can be found here. The latest version of the module can be installed using this command: pip install beautifulsoup4. The first step with Beautiful Soup is to parse the downloaded HTML into a soup document. Beautiful Soup supports several different parsers. Parsers behave differently when parsing web pages that do not contain perfectly valid HTML. For example, consider this HTML syntax of a table entry with missing attribute quotes and closing tags for the table row and table fields: &lt;tr id=students_school_row&gt; &lt;td class=w2p_fl&gt; &lt;label for=&quot;students_school&quot; id=&quot;students_school_label&quot;&gt; School: &lt;/label&gt; &lt;td class=w2p_fw&gt;IV Beautiful Soup with the lxml parser can correctly interpret the missing attribute quotes and closing tags, as well as add the &lt;html&gt; and &lt;body&gt; tags to form a complete HTML document, as the code below shows: from bs4 import BeautifulSoup broken_html = &#39;&lt;tr id=students_school_row&gt;&lt;td class=w2p_fl&gt;&lt;label for=&quot;students_school&quot; id=&quot;students_school_label&quot;&gt;School:&lt;/label&gt;&lt;td class=w2p_fw&gt;IV&#39; soup = BeautifulSoup(broken_html, &#39;lxml&#39;) fixed_html = soup.prettify() print(fixed_html) ## &lt;html&gt; ## &lt;body&gt; ## &lt;tr id=&quot;students_school_row&quot;&gt; ## &lt;td class=&quot;w2p_fl&quot;&gt; ## &lt;label for=&quot;students_school&quot; id=&quot;students_school_label&quot;&gt; ## School: ## &lt;/label&gt; ## &lt;/td&gt; ## &lt;td class=&quot;w2p_fw&quot;&gt; ## IV ## &lt;/td&gt; ## &lt;/tr&gt; ## &lt;/body&gt; ## &lt;/html&gt; But if we use the html.parser, it interprets the school name itself as a child of the school key instead of the parallel table fields and it does not create a complete HTML, as the code below shows: from bs4 import BeautifulSoup broken_html = &#39;&lt;tr id=students_school_row&gt;&lt;td class=w2p_fl&gt;&lt;label for=&quot;students_school&quot; id=&quot;students_school_label&quot;&gt;School:&lt;/label&gt;&lt;td class=w2p_fw&gt;IV&#39; soup = BeautifulSoup(broken_html, &#39;html.parser&#39;) fixed_html = soup.prettify() print(fixed_html) ## &lt;tr id=&quot;students_school_row&quot;&gt; ## &lt;td class=&quot;w2p_fl&quot;&gt; ## &lt;label for=&quot;students_school&quot; id=&quot;students_school_label&quot;&gt; ## School: ## &lt;/label&gt; ## &lt;td class=&quot;w2p_fw&quot;&gt; ## IV ## &lt;/td&gt; ## &lt;/td&gt; ## &lt;/tr&gt; However, keep it in mind that none of these parsers is always the correct way to handle invalid HTML. It will be case-by-case. The next step of using Beautiful Soup is to navigate to the elements of HTML we want using its API. Here is an example to extract the student name from our example profile webpage: from bs4 import BeautifulSoup import requests url = &#39;https://iqssdss2020.pythonanywhere.com/tutorial/static/views/Adams.html&#39; html = requests.get(url) soup = BeautifulSoup(html.text, &#39;html.parser&#39;) tr = soup.find(attrs={&#39;id&#39;:&#39;students_name_row&#39;}) td = tr.find(attrs={&#39;class&#39;:&#39;w2p_fw&#39;}) name = td.text print(name) ## Adams This code is longer than regular expressions but easier to construct and understand. Also, we no longer need to worry about problems in minor layout changes, such as extra whitespace or tag attributes. 2.1.3 Lxml The lxml module is a Python wrapper on the top of the C libraries libxml2 and libxslt. It works the same way as Beautiful Soup but is much faster. The documentation of lxml can be found here. The module can be installed using this command: pip install lxml. As with Beautiful Soup, the first step of lxml is parsing the potentially invalid HTML into a consistent format. Here is an example of parsing the same broken HTML: from lxml import etree, html broken_html = &#39;&lt;tr id=students_school_row&gt;&lt;td class=w2p_fl&gt;&lt;label for=&quot;students_school&quot; id=&quot;students_school_label&quot;&gt;School:&lt;/label&gt;&lt;td class=w2p_fw&gt;IV&#39; tree = html.fromstring(broken_html) fixed_html = etree.tostring(tree, pretty_print=True).decode(&#39;utf-8&#39;) print(fixed_html) ## &lt;tr id=&quot;students_school_row&quot;&gt; ## &lt;td class=&quot;w2p_fl&quot;&gt; ## &lt;label for=&quot;students_school&quot; id=&quot;students_school_label&quot;&gt;School:&lt;/label&gt; ## &lt;/td&gt; ## &lt;td class=&quot;w2p_fw&quot;&gt;IV&lt;/td&gt; ## &lt;/tr&gt; As with Beautiful Soup, lxml was able to correctly parse the missing attribute quotes and closing tags, although it did not add the &lt;html&gt; and &lt;body&gt; tags. Here we use lxml.etree module to formulate a more hierarchical tree structure and then convert it to text via tostring() method in order to display it. After parsing the input, lxml has its API to select elements, such as XPath selectors, like Beautiful Soup. Here is an example using the lxml xpath() method to extract the student name data: from lxml import etree, html import requests static_url = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/static/views/Adams.html&quot; static_html = requests.get(static_url) tree = html.fromstring(static_html.text) name = tree.xpath(&#39;//*[@id=&quot;students_name_row&quot;]/td[2]&#39;)[0].text print(name) ## Adams 2.1.4 Comparison of Approaches As shown in the previous sections, Beautiful Soup and lxml are more robust to webpage changes than regular expressions. Comparing their relative efficiency, lxml and the regular expression module were written in C, while Beautiful Soup is pure Python. So, lxml and regular expressions are much faster than Beautiful Soup. We did an experiment that ran each scraper to extract all the available student profile data 1000 times and record the total time taken by each scraper. A full implementation of this experiment can be found as follows, as well as the results from running this script on my computer: import re from bs4 import BeautifulSoup from lxml import html import time import requests fields = [&quot;name&quot;, &quot;school&quot;, &quot;level&quot;] def re_scraper(htmlText): results = {} for field in fields: results[field] = re.findall(&#39;&lt;tr id=&quot;students_{}_row&quot;&gt;.*?&lt;td class=&quot;w2p_fw&quot;&gt;(.*?)&lt;/td&gt;&#39;.format(field), htmlText)[0] return results def bs_scraper(htmlText): soup = BeautifulSoup(htmlText, &#39;html.parser&#39;) results = {} for field in fields: results[field] = soup.find(attrs={&#39;id&#39;:&#39;students_{}_row&#39;.format(field)}).find(attrs={&#39;class&#39;:&#39;w2p_fw&#39;}).text return results def lxml_scraper(htmlText): tree = html.fromstring(htmlText) results = {} for field in fields: results[field] = tree.xpath(&#39;//*[@id=&quot;students_{}_row&quot;]/td[2]&#39;.format(field))[0].text return results num_iterations = 1000 static_html = requests.get(&quot;https://iqssdss2020.pythonanywhere.com/tutorial/static/views/Adams.html&quot;).text for name, scraper in [(&#39;Regular Expressions&#39;, re_scraper), (&#39;Beautiful Soup&#39;, bs_scraper), (&#39;Lxml&#39;, lxml_scraper)]: start = time.time() for i in range(num_iterations): if scraper == re_scraper: re.purge() result = scraper(static_html) assert(result[&quot;name&quot;] == &quot;Adams&quot;) assert(result[&quot;school&quot;] == &quot;IV&quot;) assert(result[&quot;level&quot;] == &quot;No&quot;) end = time.time() print(&#39;{}: {} seconds&#39;.format(name, end - start)) ## Regular Expressions: 0.3739955425262451 seconds ## Beautiful Soup: 0.8199987411499023 seconds ## Lxml: 0.13904094696044922 seconds The results show that Beautiful Soup is much slower than the other two approaches. Regular expressions does not perform the fastest, because we call re.purge() in every iteration to clear cache. By default, the regular expression module will cache searches and this cache needs to be cleared to make a fair comparison with the other scraping approaches. lxml performs comparatively well with regular expressions, although lxml has the additional overhead of having to parse the input into its internal format before searching for elements. When scraping many features from a web page, this initial parsing overhead is reduced and lxml becomes even more competitive. 2.2 Approaches to Scraping a Dynamic Web Page There are two approaches to scraping a dynamic webpage: scrape the content directly from the JavaScript, or use Python packages capable of executing the JavaScript itself, and scrape the website as you view it in your browser. 2.2.1 AJAX Requests Because the data is loaded dynamically with JavaScript, to scrape this data, we need to understand how the web page loads this data. Suppose that we want to find all students whose names start with letter A in the fifth grade with page size set at 5 from this example dynamic web page. After we click Search button, open Fiddler—a software that can inspect HTTP requests on your computer and can be downloaded here. We will see that an AJAX request is made. Under Request Headers in the Inspectors window, we can find the URL for this search. Under Response window, we can see the response content is in JSON format. They are highlighted in blue in the figure as follows: AJAX stands for Asynchronous JavaScript and XML. A dynamic web page works because the AJAX allows JavaScript to make HTTP requests to a remote server and receive responses. This approach is to first access to the AJAX request responses, and then to scrape information of interest from them. The AJAX response data can be downloaded directly. With the URL of the response, we can make a request to the server, scrape the information from the response, and store the scraped information in a spreadsheet, as the following code shows: import requests import pandas as pd html = requests.get(&#39;https://iqssdss2020.pythonanywhere.com/tutorial/cases/search_ajax?search_name=A&amp;search_grade=5&amp;page_size=5&amp;page=1&#39;) html_json = html.json() print(html_json) ## {&#39;records&#39;: [{&#39;Name&#39;: &#39;Annie&#39;, &#39;Grade&#39;: &#39;5&#39;, &#39;GPA&#39;: 3.0}, {&#39;Name&#39;: &#39;Ala&#39;, &#39;Grade&#39;: &#39;5&#39;, &#39;GPA&#39;: 2.5}, {&#39;Name&#39;: &#39;Aayusha&#39;, &#39;Grade&#39;: &#39;5&#39;, &#39;GPA&#39;: 3.5}, {&#39;Name&#39;: &#39;Anushri&#39;, &#39;Grade&#39;: &#39;5&#39;, &#39;GPA&#39;: 4.0}, {&#39;Name&#39;: &#39;Andrew&#39;, &#39;Grade&#39;: &#39;5&#39;, &#39;GPA&#39;: 3.0}], &#39;num_pages&#39;: 5, &#39;error&#39;: &#39;&#39;} students_A5p0 = pd.DataFrame.from_records(html_json[&#39;records&#39;]) print(students_A5p0.head(10)) ## GPA Grade Name ## 0 3.0 5 Annie ## 1 2.5 5 Ala ## 2 3.5 5 Aayusha ## 3 4.0 5 Anushri ## 4 3.0 5 Andrew Here is an example implementation that scrapes all the students by searching for each letter of the alphabet and each grade, and then iterating the resulting pages of the JSON responses. The results are then stored in a spreadsheet. import requests import pandas as pd import string temp_url = &#39;https://iqssdss2020.pythonanywhere.com/tutorial/cases/search_ajax?search_name={}&amp;search_grade={}&amp;page_size=5&amp;page={}&#39; students = list() grades = [&quot;K&quot;, &quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;] for letter in string.ascii_uppercase: for grade in grades: page = 0 while True: url = temp_url.format(letter, grade, page) html = requests.get(url) html_json = html.json() students.extend(html_json[&quot;records&quot;]) page += 1 if page &gt;= html_json[&quot;num_pages&quot;]: break students_df = pd.DataFrame.from_records(students) print(students_df.head(10)) ## GPA Grade Name ## 0 3 1 Allen ## 1 3.5 4 Anderson ## 2 4 5 Adams ## 3 1 5 Alexander ## 4 3 5 Aaron ## 5 3.5 5 Aws ## 6 2 5 Alan ## 7 3 5 Annie ## 8 2.5 5 Ala ## 9 3.5 5 Aayusha The AJAX-dependent websites initially look more complex but their structure encourages separating the data transmission between client and server and the data presentation on the client browser executing JavaScript, which can make our job of extracting this data much easier. 2.2.2 Selenium The second approach uses Python packages capable of executing the JavaScript itself, and scrape the website as you view it in your browser. Selenium works by automating browsers to execute JavaScript to display a web page as we expect. To confirm that Selenium can automate browser to execute JavaScript, this is a simple example web page. This web page simply uses JavaScript to write a table to a div element. Here is the source code: &lt;html&gt; &lt;body&gt; &lt;div id=&quot;result&quot;&gt;&lt;/div&gt; &lt;script&gt; document.getElementById(&quot;result&quot;).innerHTML = `&lt;table&gt; &lt;tr&gt; &lt;th&gt;Name&lt;/th&gt; &lt;th&gt;Grade&lt;/th&gt; &lt;th&gt;GPA&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Adams&lt;/td&gt; &lt;td&gt;5&lt;/td&gt; &lt;td&gt;4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Alexander&lt;/td&gt; &lt;td&gt;5&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Aaron&lt;/td&gt; &lt;td&gt;5&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Aws&lt;/td&gt; &lt;td&gt;5&lt;/td&gt; &lt;td&gt;3.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Alan&lt;/td&gt; &lt;td&gt;5&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; `; &lt;/script&gt; &lt;/body&gt; &lt;/html&gt; With the traditional approach of downloading the original HTML and parsing the result, the div element will be empty, as follows: from lxml import html import requests global_dynamicUrl = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/default/dynamic&quot; global_dynamicPage = requests.get(global_dynamicUrl) global_dynamicHtml = html.fromstring(global_dynamicPage.text) table_area = global_dynamicHtml.xpath(&#39;//*[@id=&quot;result&quot;]/table&#39;) print(table_area) ## [] Here is an initial example with Selenium. Selenium can be installed using pip with the command: pip install selenium. The first step is to create a connection to the web browser that you use. Next is to load a web page in the chosen web browser via executing the JavaScript. The JavaScript is executed because now the div element has an object representing a table, and within that object, there are 6 objects representing 6 table entries. from selenium import webdriver driver = webdriver.Chrome(&#39;https://driver/chromedriver.exe&#39;) global_dynamicUrl = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/default/dynamic&quot; driver.get(global_dynamicUrl) table_area = driver.find_element_by_xpath(&#39;//*[@id=&quot;result&quot;]/table&#39;) table_entries = table_area.find_elements_by_tag_name(&quot;tr&quot;) print(len(table_entries)) driver.close() So far, our browser automaton can only execute JavaScript and access the resulting HTML. To scrape the resulting HTML will require extending the browser automation to support intensive website interactions with the user. Fortunately, Selenium has an excellent API to select and manipulate the HTML elements, which makes this straightforward. Here is an example implementation that rewrites the previous search all students example in Selenium. We will cover Selenium in detail in the following chapters. from selenium import webdriver import time import string import pandas as pd driver = webdriver.Chrome(&#39;https://driver/chromedriver.exe&#39;) searchAddress = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/cases/search&quot; driver.get(searchAddress) time.sleep(2) students = list() for letter in string.ascii_uppercase: for grade in range(2,8): driver.find_element_by_xpath(&#39;//*[@id=&quot;search_name&quot;]&#39;).send_keys(letter) driver.find_element_by_xpath(&#39;//*[@id=&quot;search_grade&quot;]/option[{}]&#39;.format(grade)).click() driver.find_element_by_xpath(&#39;//*[@id=&quot;search&quot;]&#39;).click() time.sleep(5) try: while True: table = driver.find_element_by_xpath(&#39;//*[@id=&quot;results&quot;]/table&#39;) entries = table.find_elements_by_tag_name(&quot;tr&quot;) for i in range(1, len(entries)): student_dict = dict() cols = entries[i].find_elements_by_tag_name(&quot;td&quot;) student_dict[&quot;name&quot;] = cols[0].text student_dict[&quot;grade&quot;] = cols[1].text student_dict[&quot;gpa&quot;] = cols[2].text students.append(student_dict) try: driver.find_element_by_xpath(&#39;//*[@id=&quot;next&quot;]&#39;).click() time.sleep(2) except: break driver.get(searchAddress) time.sleep(2) except: print(&quot;No results for letter {} at grade {}&quot;.format(letter, grade - 2)) driver.get(searchAddress) time.sleep(2) students_df = pd.DataFrame.from_records(students) print(students_df.head(10)) driver.close() 2.2.3 Comparison of Approaches Because the first approach needs to understand how the data is loaded dynamically with JavaScript, it needs you to understand the JavaScript code, which can be found in View page source. For the example search web page, we were able to easily know how it works. However, some websites will be very complex and difficult to understand. With enough effort, any website can be scraped in this way. However, this effort can be avoided by instead using a Python package Selenium that automates a web browser to execute JavaScript to display a web page as we expect and then perform actions on this web page. Doing this way only needs you to know how Selenium works and its APIs that control a web browser. You do not need to understand how the backend of a website works. However, there are disadvantages. Automating a web browser adds overhead and so is much slower than just downloading the HTML. Additionally, solutions using a browser driver often require polling the web page to check whether the resulting HTML from an event has occurred yet or waiting a set amount of time for an AJAX event is complete by then, which is brittle and can easily fail when the network is slow. "],
["when-to-use-selenium-driver.html", "3 When to Use Selenium Driver? 3.1 An Example of Dynamic Search 3.2 An Example of Dynamic Link 3.3 An Example of Dynamic New Load", " 3 When to Use Selenium Driver? The Selenium driver is particularly used to scrape data from dynamic websites that use JavaScript (although it can scrape data from static websites too). The use of JavaScript can vary from simple form events to single page apps that download all their content after loading. The consequence of this is that for many web pages the content that is displayed in our web browser is not available in the original HTML. For example, the result table shows up only after the user click the search box, the content following a click on a link is generated instantaneously rather than already being stored on the server before the click, or a JavaScript request might trigger a new block of content to load. The following subsections of this part cover those three examples in detail. This tutorial will cover the scraping techniques that use Python Selenium package capable of executing the JavaScript itself, and scrape the website as you view it in your browser. 3.1 An Example of Dynamic Search Let us look at an example dynamic web page, which is available at Dynamic Search. This example website has a search form that is used to locate students. Let us say we want to find all the students whose name begins with the letter A and who are in the fifth grade: Figure I-1.1 We place the mouse anywhere on this webpage. We right-click the mouse and click inspect from the menu to inspect the results. In the Elements window, we would find that the results are stored within a &lt;div&gt; element with ID “results”: Figure I-1.2 Let us try to scrape the information from the result table using the lxml module, which was also covered in detail in the Python Web-Scraping Workshop. from lxml import html import requests search_url = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/cases/search&quot; search_page = requests.get(search_url) search_html = html.fromstring(search_page.text) firstEntry_link = search_html.xpath(&#39;//*[@id=&quot;results&quot;]/table&#39;) print(firstEntry_link) ## [] The example scraper here has failed to extract results since the xpath() method returns an empty list. Examining the source code of this web page can help you understand why. Let us put the mouse anywhere on this webpage. Right-click the mouse and click View page source from the menu to examine the source code. Here, we find that the &lt;div&gt; element with ID “results” is empty. Figure I-1.3 If we scroll down the source code a little bit, we can find that the display of the result table is coded in a JavaScript function called as displayResult(jsonresult) in the JavaScript section. This means that the web page has used JavaScript to load the search results dynamically. At this point, you could recognize that page source code is the original HTML you get when you make a request to the server without executing JavaScript code. In contrast, the Elements window has the HTML that has been revised via running the JavaScript section code. Figure I-1.4 3.2 An Example of Dynamic Link Let us now examine what a dynamic link is, and how it is different from a static link. When you click on a link, if it is a static link, the content that appears comes from a file that has been stored on the server before the click. If it is a dynamic link, then the content that appears is generated instantaneously after the click by executing a JavaScript function. Then let us take a further look about how different a dynamic link is from a static link. Let us still inspect the result table in the Elements window. Look at the two links highlighted in Figure I-2.1. These two links have a tag name &lt;a&gt; with an attribute href. The first link is a static link, while the second is a dynamic link. Figure I-2.1 Normally, if a link is a static link, its link address will end with a file name with a file extension. In this case, the link address (https://iqssdss2020.pythonanywhere.com/tutorial/static/views/Adrian.html) ends with a .html file. The link address is formed first starting with a server address (https://iqssdss2020.pythonanywhere.com) and followed by the web application name (/tutorial) and then followed by a hierarchical path of folders to that .html file. Figure I-2.2 On the other hand, if a link is a dynamic link, you will not find a file extension at the end of its link address. For example, in this case, the link address (https://iqssdss2020.pythonanywhere.com/tutorial/cases/getstudent/Adams) has no a file extension at the end. It is formed first beginning at a server address (https://iqssdss2020.pythonanywhere.com) and followed by the web application name (/tutorial) and then followed by the name of the Python script (cases.py) in the web application (/cases) that calls a function in this script called as getstudent() (/getstudent) followed with a parameter value (/Adams) to the getstudent() function. Figure I-2.3 Suppose that we temporarily disable the JavaScript execution functionality in our browser by changing the browser’s settings. Let us copy the static link address and open it in a new browser. We should be able to see the profile of the chosen student as Figure 1-2.4 shows. But if we copy the dynamic link address and open it in a new browser, we will only see an empty webpage with just page layout as Figure 1-2.5 shows. This is because the new browser whose JavaScript functionality has been disabled cannot execute JavaScript code in order to display the profile content. Figure I-2.4 Figure I-2.5 We can also see this difference using the requests module, which was also covered in detail in the Python Web-Scraping Workshop. staticLink_url = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/static/views/Adams.html&quot; staticLink_page = requests.get(staticLink_url) staticLink_html = html.fromstring(staticLink_page.text) html.open_in_browser(staticLink_html, encoding = &#39;UTF-8&#39;) ## file://C:/Users/JLiu/AppData/Local/Temp/tmpj2t2fmk4.html dynamicLink_url = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/cases/getstudent/Adams&quot; dynamicLink_page = requests.get(dynamicLink_url) dynamicLink_html = html.fromstring(dynamicLink_page.text) html.open_in_browser(dynamicLink_html, encoding = &#39;UTF-8&#39;) ## file://C:/Users/JLiu/AppData/Local/Temp/tmpxi4_1zc2.html The requests module cannot execute JavaScript code. As the code above illustrates, it behaves the same as a browser whose JavaScript functionality is disabled. Examining the source code of the chosen student’s profile web page of both the static and dynamic links can help you further understand why. The source code of the profile web page of a static link shows a complete html file without a JavaScript section. Figure I-2.6 In the source code of the profile web page of a dynamic link, we find that the &lt;div&gt; element with ID “results” is empty. There is a JavaScript section next to it. We can find that in this section there is a JavaScript page load function. This function makes a request to the server for the chosen student’s profile. The server then returns the information to the function. If successful, this function then displays the profile. This means that the web page has used JavaScript to load the chosen student’s profile dynamically. Figure I-2.7 3.3 An Example of Dynamic New Load The dynamic load means that the new content appears only after a JavaScript request for that information is made to the server. There are two major ways in web application design to make a JavaScript request in order to trigger a new block of content to load. The first way uses pagination, and the second way scrolls bar to the bottom of a page. Let us first look at pagination. Let us inspect the web page of dynamic search again. We would find that the page links are stored within a &lt;div&gt; element with ID “pagination”. Here, the href attribute has a value of javascript:void(0). It just means the browser stays at the same position on the same page and does nothing. Once the page links are clicked, the browser will execute a JavaScript function previous() / next() to make another JavaScript request to the server for the information on that page and then display those new information coming from the server on the previous or next page via executing the relevant JavaScript functions. Figure I-3.1 In this case, the value of the href attribute is not an URL (The Uniform Resource Locator). So, there is no point to try to test if this is a static or dynamic link using the requests module as what we do in Section I-2. But we can illustrate the dynamic load using the lxml module. The code below tries to scrape the page link information using the lxml module. The scraper here has failed to extract the page links since the xpath() method returns an empty list. search_url = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/cases/search&quot; search_page = requests.get(search_url) search_html = html.fromstring(search_page.text) page_link = search_html.xpath(&#39;//*[@id=&quot;next&quot;]&#39;) print(page_link) ## [] Let us examine the page source code to see why. Here, we find that the &lt;div&gt; element with ID “pagination” is empty. If we scroll down the source code to the end, we can find that the display of the page links is coded in a JavaScript function displayResult(jsonresult) in the JavaScript section. This means that the web page has used JavaScript to load the page links and insert it at the position of the &lt;div&gt; element with ID “pagination” in the original HTML. We could see the revised HTML after running the JavaScript code in the Elements window. Figure I-3.2 Figure I-3.3 Now let us examine the second way of design – scroll down to the bottom of page – to load the new content. Let us look at another example web page, which is available at dynamic search load. This webpage is the same as the previous example webpage except it uses scrolling bar down instead of clicking page link to load the new content. The code below tries to scrap the information of the result table’s entries using the lxml module. The scraper here has failed to extract those information since the xpath() method returns an empty list. searchLoad_url = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/casesLoad/search&quot; searchLoad_page = requests.get(searchLoad_url) searchLoad_html = html.fromstring(searchLoad_page.text) entries_link = searchLoad_html.xpath(&#39;//*[@id=&quot;resultstable&quot;]/tbody/tr&#39;) print(page_link) ## [] Figure I-3.4 illustrates the result table part of code in the original HTML from the page source code. Figure I-3.5 highlights the same part of code in the revised HTML from the Elements window after it executes the JavaScript code. It is clear to see that there is no information under the tag name &lt;tbody&gt; in Figure I-3.4. This explains why the xpath() method returns an empty list. In Figure I-3.5, the webpage runs the JavaScript to insert the first chunk of the students’ information into the empty result table that has been created statically before running the JavaScript and then display it in the browser. Figure I-3.4 Figure I-3.5 More interestingly, once we open the Elements window of this example webpage, under the &lt;tbody&gt; tag, we find there are 15 table entries (with tag &lt;tr&gt;). We can therefore infer that a load has a total of 15 table entries. If we scroll down to the bottom of the webpage, we could see that 9 more entries are appended to the table. If we continue to scroll down to the bottom of the page, nothing is changed. This means that there exists a total of 24 table entries and the new load loads the next 9 new ones after the first batch. Your browser executes the JavaScript code to perform all these actions. If you look at the JavaScript section in the page source code, you will see that a JQuery method $(window).on(\"scroll\", function() defines the scrolling down to the bottom of page and triggers the load of the new content once it is satisfied. And another JQuery method $('#resultstable &gt; tbody:last-child').append(htmlContent) append the new entries to the result table. Here the screenshot of the relevant parts of the JavaScript code is not shown. "]
]

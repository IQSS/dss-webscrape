[
["index.html", "Web Scraping Using Selenium Python Introduction Table of Contents Authors and Sources", " Web Scraping Using Selenium Python August 2020 Introduction Table of Contents In this tutorial, we first provide an overview of some foundational concepts about the world-wide-web. We then lay out some common approaches to web scraping and compare their usage. With this background, we introduce several applications that use the Selenium Python package to scrape websites. This tutorial is organized into the following parts: Basic concepts of the world-wide-web. Comparison of some common approaches to web scraping. Use-cases for when to use the Selenium Driver. Illustration of how to find web elements using Selenium Driver. Illustration of how to fill in web forms using Selenium Driver. We plan to add more applications in the near future. The content of this tutorial is a work in progress, and we are happy to receive feedback! If you find anything confusing or think the guide misses important content, please email: help@iq.harvard.edu. Authors and Sources Jinjie Liu at IQSS designed the structure of the guide and created the content. Steve Worthington at IQSS helped design the structure of the guide and edited the content. We referenced the following sources when we wrote this guide: Web Scraping with Python: Scrape data from any website with the power of Python, by Richard Lawson (ISBN: 978-1782164364) Web Scraping with Python: Collecting Data From the Modern Web, by Ryan Mitchell (ISBN: 978-1491910276) Hands-on Web Scraping with Python: Perform advanced scraping operations using various Python libraries and tools such as Selenium, Regex, and others, by Anish Chapagain (ISBN: 978-1789533392) Learning Selenium Testing Tools with Python: A practical guide on automated web testing with Selenium using Python, by Unmesh Gundecha (ISBN: 978-1783983506) "],
["concepts.html", "1 Concepts 1.1 How does the web work? 1.2 Uniform Resource Locator (URL) 1.3 Document Object Model (DOM)", " 1 Concepts We start with the basic concepts that are generic to all web scraping approaches. 1.1 How does the web work? 1.1.1 Components Computers connected to the web are called clients and servers. A simplified diagram of how they interact might look like this: Clients are the typical web user’s internet-connected devices (for example, a computer connected to Wi-Fi) and web-accessing software available on those devices (usually a web browser like Firefox or Chrome). Servers are computers that store webpages, sites, or apps. When a client device wants to access a webpage, a copy of the webpage is downloaded from the server onto the client machine to be displayed in the user’s web browser. HTTP is a language for clients and servers to speak to each other. 1.1.2 So what happens? When we type a web address into our browser: The browser finds the address of the server that the website lives on. The browser sends an HTTP request message to the server, asking it to send a copy of the website to the client. If the server approves the client’s request, the server sends the client a 200 OK message, and then starts displaying the website in the browser. 1.2 Uniform Resource Locator (URL) To retrieve information from the website (i.e., make a request), we need to know the location of the information we want to collect. The Uniform Resource Locator (URL) — commonly know as a “web address”, specifies the location of a resource (such as a web page) on the internet. A URL is usually composed of 5 parts: The 4th part, the “query string”, contains one or more parameters — in this case, there are two parameters: id and cat. The 5th part, the “fragment”, is an internal page reference and may not be present. It is often convenient to create variables containing the domain(s) and path(s) we’ll be working with, as this allows us to swap out paths and parameters as needed. Note that the path is separated from the domain with a forward slash / and the parameters are separated from the path with a question mark ?. If there are multiple parameters they are separated from each other with an ampersand &amp;. 1.3 Document Object Model (DOM) To parse HTML, it is convenient to represent our HTML document as a tree-like structure that contains information in nodes and links information through branches. This tree-like structure is called the Document Object Model (DOM). DOM is a cross-platform and language-independent interface that treats an XML or HTML document as a tree structure wherein each node is an object representing a part of the document. Each branch of the tree ends in a node, and each node contains objects. DOM methods allow programmatic access to the tree; with them one can change the structure, style or content of a document. The following schematic is an example of DOM hierarchy in an HTML document: Below is an example of DOM hierarchy in a working segment of HTML code. Here, we create a spreadsheet using the &lt;table&gt; element, which has row elements &lt;tr&gt;, header elements &lt;th&gt;, and cell elements &lt;td&gt;. In this case, we store the Name, Grade, and GPA of two students: “Adam”, and “Alexander”. &lt;html&gt; &lt;body&gt; &lt;div id=&quot;My_table&quot;&gt;&lt;/div&gt; &lt;table&gt; &lt;tr&gt; &lt;th&gt;Name&lt;/th&gt; &lt;th&gt;Grade&lt;/th&gt; &lt;th&gt;GPA&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Adams&lt;/td&gt; &lt;td&gt;5&lt;/td&gt; &lt;td&gt;4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Alexander&lt;/td&gt; &lt;td&gt;5&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/body&gt; &lt;/html&gt; "],
["web-scraping-approaches.html", "2 Web Scraping Approaches 2.1 Decision tree 2.2 Static web pages 2.3 Dynamic web pages", " 2 Web Scraping Approaches 2.1 Decision tree There are many commonly used web scraping approaches. This decision tree will help us to decide upon the best approach to use for a particular web site. If the content we are viewing in our browser does not match the content we see in the HTML source code we are retrieving from the site, then we are encountering a dynamic website. Otherwise, if the browser and source code content match each other, the website is static. A mismatch of content would be due to the execution of JavaScript that changes the HTML elements on the page. Using the Chrome browser, we can view the original HTML via View page source. We can view the revised HTML in our browser if it executes JavaScript in the Elements window via Inspect the web page. 2.2 Static web pages There are three approaches to extracting data from a static webpage that has been downloaded. We can use: Regular expressions Beautiful Soup Python module lxml Python module We use this static student profile webpage to provide examples for each approach. Suppose that we want to scrape a student name. The data we are interested in is found in the following part of the HTML. The student name is included within a &lt;td&gt; element of class=\"w2p_fw\", which is the child of a &lt;tr&gt; element of ID students_name_row. &lt;table&gt; &lt;tr id=&quot;students_name_row&quot;&gt;&lt;td class=&quot;w2p_fl&quot;&gt;&lt;label for=&quot;students_name&quot; id=&quot;students_name_label&quot;&gt;Name:&lt;/label&gt;&lt;/td&gt;&lt;td class=&quot;w2p_fw&quot;&gt;Adams&lt;/td&gt; &lt;td class=&quot;w2p_fc&quot;&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr id=&quot;students_school_row&quot;&gt;&lt;td class=&quot;w2p_fl&quot;&gt;&lt;label for=&quot;students_school&quot; id=&quot;students_school_label&quot;&gt;School:&lt;/label&gt;&lt;/td&gt;&lt;td class=&quot;w2p_fw&quot;&gt;IV&lt;/td&gt; &lt;td class=&quot;w2p_fc&quot;&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr id=&quot;students_level_row&quot;&gt;&lt;td class=&quot;w2p_fl&quot;&gt;&lt;label for=&quot;students_level&quot; id=&quot;students_level_label&quot;&gt;Advanced:&lt;/label&gt;&lt;/td&gt;&lt;td class=&quot;w2p_fw&quot;&gt;No&lt;/td&gt; &lt;td class=&quot;w2p_fc&quot;&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; 2.2.1 Regular expressions Regular expressions (regex) directly work on a downloaded web page, without any need to parse the page into a certain format. We can use regex to match the content we want to extract from the HTML. There is a thorough overview of regex here. In this example, we need to match the &lt;td class=\"w2p_fw\"&gt; tag to scrape the student name. But this tag is used for multiple student profile attributes. To isolate the name, we select the first element, as shown in the code below: import re import requests url = &#39;https://iqssdss2020.pythonanywhere.com/tutorial/static/views/Adams.html&#39; html = requests.get(url) mylist = re.findall(&#39;&lt;td class=&quot;w2p_fw&quot;&gt;(.*?)&lt;/td&gt;&#39;, html.text) print(mylist) name = re.findall(&#39;&lt;td class=&quot;w2p_fw&quot;&gt;(.*?)&lt;/td&gt;&#39;, html.text)[0] print(name) This solution works, but can easily fail if the web page is updated later. Consider if the student ID data is inserted right before the student name. Then we must change the code to select the second element. The general solution to make a regular expression scraper more robust is to include the parent element, which has an ID, so it ought to be unique: import re import requests url = &#39;https://iqssdss2020.pythonanywhere.com/tutorial/static/views/Adams.html&#39; html = requests.get(url) mylist = re.findall(&#39;&lt;tr id=&quot;students_name_row&quot;&gt;&lt;td class=&quot;w2p_fl&quot;&gt;&lt;label for=&quot;students_name&quot; id=&quot;students_name_label&quot;&gt;Name:\\ &lt;/label&gt;&lt;/td&gt;&lt;td class=&quot;w2p_fw&quot;&gt;(.*?)&lt;/td&gt;&#39;, html.text) print(mylist) This solution is better. However, there are many other ways the web page can be updated that still break the regex. For example, double quotation might be changed to single quotation for class name, extra space could be added between the &lt;td&gt; tags, or the students_name_label could be changed. The general solution for this is to make the regex as generic as possible to support various possibilities: import re import requests url = &#39;https://iqssdss2020.pythonanywhere.com/tutorial/static/views/Adams.html&#39; html = requests.get(url) mylist = re.findall(&#39;&lt;tr id=&quot;students_name_row&quot;&gt;.*?&lt;td\\s*class=[&quot;\\&#39;]w2p_fw[&quot;\\&#39;]&gt;(.*?)&lt;/td&gt;&#39;, html.text) print(mylist) This regex is more robust to webpage updates but is more difficult to construct, becoming even unreadable. But still, there are other minor layout changes that would break it, such as if a title attribute is added to the &lt;td&gt; tag. From this example, we can see that regex provide a quick way to scrape data without the step of parsing, but are too brittle and will easily break when a web page is updated. 2.2.2 Beautiful soup Beautiful Soup is a popular Python module that parses a downloaded web page into a certain format and then provides a convenient interface to navigate content. The official documentation of Beautiful Soup can be found here. The latest version of the module can be installed using this command: pip install beautifulsoup4. The first step with Beautiful Soup is to parse the downloaded HTML into a “soup document”. Beautiful Soup supports several different parsers. Parsers behave differently when parsing web pages that do not contain perfectly valid HTML. For example, consider this HTML syntax of a table entry with missing attribute quotes and closing tags for the table row and table fields: &lt;tr id=students_school_row&gt; &lt;td class=w2p_fl&gt; &lt;label for=&quot;students_school&quot; id=&quot;students_school_label&quot;&gt; School: &lt;/label&gt; &lt;td class=w2p_fw&gt;IV Beautiful Soup with the lxml parser can correctly interpret the missing attribute quotes and closing tags, as well as add the &lt;html&gt; and &lt;body&gt; tags to form a complete HTML document, as the code below shows: from bs4 import BeautifulSoup broken_html = &#39;&lt;tr id=students_school_row&gt;&lt;td class=w2p_fl&gt;&lt;label for=&quot;students_school&quot; id=&quot;students_school_label&quot;&gt;School:&lt;/label&gt;&lt;td class=w2p_fw&gt;IV&#39; soup = BeautifulSoup(broken_html, &#39;lxml&#39;) fixed_html = soup.prettify() print(fixed_html) But if we use the html.parser, it interprets the school name itself as a child of the school key instead of the parallel table fields and it does not create a complete HTML document, as the code below shows: from bs4 import BeautifulSoup broken_html = &#39;&lt;tr id=students_school_row&gt;&lt;td class=w2p_fl&gt;&lt;label for=&quot;students_school&quot; id=&quot;students_school_label&quot;&gt;School:&lt;/label&gt;&lt;td class=w2p_fw&gt;IV&#39; soup = BeautifulSoup(broken_html, &#39;html.parser&#39;) fixed_html = soup.prettify() print(fixed_html) However, keep in mind that none of these parsers represent a universal solution to the problem of invalid HTML. Solutions will have to be found on case-by-case basis. The next step of using Beautiful Soup is to navigate to the elements of HTML we want using its application programming interface (API). Here is an example to extract the student name from our example profile webpage: from bs4 import BeautifulSoup import requests url = &#39;https://iqssdss2020.pythonanywhere.com/tutorial/static/views/Adams.html&#39; html = requests.get(url) soup = BeautifulSoup(html.text, &#39;html.parser&#39;) tr = soup.find(attrs={&#39;id&#39;:&#39;students_name_row&#39;}) td = tr.find(attrs={&#39;class&#39;:&#39;w2p_fw&#39;}) name = td.text print(name) This code is longer than the regex example, but easier to construct and understand. Also, we no longer need to worry about problems in minor layout changes, such as extra whitespace or tag attributes. 2.2.3 Lxml The lxml module is a Python wrapper on the top of the C libraries libxml2 and libxslt. It works the same way as Beautiful Soup, but is much faster. Here is the documentation for lxml. The module can be installed using this command: pip install lxml. As with Beautiful Soup, the first step of lxml is parsing the potentially invalid HTML into a consistent format. Here is an example of parsing the same broken HTML: from lxml import etree, html broken_html = &#39;&lt;tr id=students_school_row&gt;&lt;td class=w2p_fl&gt;&lt;label for=&quot;students_school&quot; id=&quot;students_school_label&quot;&gt;School:&lt;/label&gt;&lt;td class=w2p_fw&gt;IV&#39; tree = html.fromstring(broken_html) fixed_html = etree.tostring(tree, pretty_print=True).decode(&#39;utf-8&#39;) print(fixed_html) As with Beautiful Soup, lxml was able to correctly parse the missing attribute quotes and closing tags, although it did not add the &lt;html&gt; and &lt;body&gt; tags. Here we use the lxml.etree module to formulate a more hierarchical tree structure and then convert it to text via the tostring() method to display it. After parsing the input, lxml has its API to select elements, such as XPath selectors, like Beautiful Soup. Here is an example using the lxml xpath() method to extract the student name data: from lxml import etree, html import requests static_url = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/static/views/Adams.html&quot; static_html = requests.get(static_url) tree = html.fromstring(static_html.text) name = tree.xpath(&#39;//*[@id=&quot;students_name_row&quot;]/td[2]&#39;)[0].text print(name) 2.2.4 Comparison of approaches As shown in the previous sections, Beautiful Soup and lxml are more robust to webpage changes than regex. Comparing their relative efficiency, lxml and the regex module were written in C, while Beautiful Soup is pure Python. So, lxml and regex are much faster than Beautiful Soup. To provide benchmarks for these approaches, here we construct an experiment to run each scraper to extract all the available student profile data 1000 times and record the total time taken by each scraper: import re from bs4 import BeautifulSoup from lxml import html import time import requests fields = [&quot;name&quot;, &quot;school&quot;, &quot;level&quot;] def re_scraper(htmlText): results = {} for field in fields: results[field] = re.findall(&#39;&lt;tr id=&quot;students_{}_row&quot;&gt;.*?&lt;td class=&quot;w2p_fw&quot;&gt;(.*?)&lt;/td&gt;&#39;.format(field), htmlText)[0] return results def bs_scraper(htmlText): soup = BeautifulSoup(htmlText, &#39;html.parser&#39;) results = {} for field in fields: results[field] = soup.find(attrs={&#39;id&#39;:&#39;students_{}_row&#39;.format(field)}).find(attrs={&#39;class&#39;:&#39;w2p_fw&#39;}).text return results def lxml_scraper(htmlText): tree = html.fromstring(htmlText) results = {} for field in fields: results[field] = tree.xpath(&#39;//*[@id=&quot;students_{}_row&quot;]/td[2]&#39;.format(field))[0].text return results num_iterations = 1000 static_html = requests.get(&quot;https://iqssdss2020.pythonanywhere.com/tutorial/static/views/Adams.html&quot;).text for name, scraper in [(&#39;Regular Expressions&#39;, re_scraper), (&#39;Beautiful Soup&#39;, bs_scraper), (&#39;Lxml&#39;, lxml_scraper)]: start = time.time() for i in range(num_iterations): if scraper == re_scraper: re.purge() result = scraper(static_html) assert(result[&quot;name&quot;] == &quot;Adams&quot;) assert(result[&quot;school&quot;] == &quot;IV&quot;) assert(result[&quot;level&quot;] == &quot;No&quot;) end = time.time() print(&#39;{}: {} seconds&#39;.format(name, end - start)) The results, when run on a modest Windows desktop computer, show that Beautiful Soup is much slower than the other two approaches. Regex does not perform the fastest, because we call re.purge() in every iteration to clear the cache. By default, the regex module will cache searches and this cache needs to be cleared to make a fair comparison with the other scraping approaches. The lxml module performs comparatively well with regex, although lxml has the additional overhead of having to parse the input into its internal format before searching for elements. When scraping many features from a web page, this initial parsing overhead is reduced and lxml becomes even more competitive. 2.3 Dynamic web pages There are two approaches to scraping a dynamic webpage: Scrape the content directly from the JavaScript Scrape the website as we view it in our browser — using Python packages capable of executing the JavaScript. 2.3.1 AJAX requests Because the data are loaded dynamically with JavaScript, to scrape these data, we need to understand how the web page loads the data. Suppose that we want to find all students whose names start with the letter “A” in the fifth grade with page size set at 5 from this example dynamic web page. After we click the Search button, open Fiddler –— software that can inspect HTTP requests on the computer and can be downloaded here. We will see that an AJAX request is made. Under Request Headers in the Inspector window, we can find the URL for this search. Under the Response window, we can see the response content is in JSON format. They are highlighted in blue in the following figure: AJAX stands for Asynchronous JavaScript and XML. A dynamic web page works because the AJAX allows JavaScript to make HTTP requests to a remote server and receive responses. This approach works by first accessing the AJAX request responses, and then scraping information of interest from them. The AJAX response data can be downloaded directly. With the URL of the response, we can make a request to the server, scrape the information from the response, and store the scraped information in a spreadsheet, as the following code shows: import requests import pandas as pd html = requests.get(&#39;https://iqssdss2020.pythonanywhere.com/tutorial/cases/search_ajax?search_name=A&amp;search_grade=5&amp;page_size=5&amp;page=1&#39;) html_json = html.json() print(html_json) students_A5p0 = pd.DataFrame.from_records(html_json[&#39;records&#39;]) print(students_A5p0.head(10)) Here is an example implementation that scrapes all the students by searching for each letter of the alphabet and each grade, and then iterating over the resulting pages of the JSON responses. The results are then stored in a spreadsheet: import requests import pandas as pd import string temp_url = &#39;https://iqssdss2020.pythonanywhere.com/tutorial/cases/search_ajax?search_name={}&amp;search_grade={}&amp;page_size=5&amp;page={}&#39; students = list() grades = [&quot;K&quot;, &quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;] for letter in string.ascii_uppercase: for grade in grades: page = 0 while True: url = temp_url.format(letter, grade, page) html = requests.get(url) html_json = html.json() students.extend(html_json[&quot;records&quot;]) page += 1 if page &gt;= html_json[&quot;num_pages&quot;]: break students_df = pd.DataFrame.from_records(students) print(students_df.head(10)) The AJAX-dependent websites initially look more complex but their structure encourages separating the data transmission between client and server and the data presentation on the client browser executing JavaScript, which can make our job of extracting these data much easier. 2.3.2 Selenium The second approach to scraping dynamic web pages uses Python packages capable of executing the JavaScript itself, so that we can scrape the website as we view it in our browser. Selenium works by automating browsers to execute JavaScript to display a web page as we would normally interact with it. To illustrate how Selenium can automate a browser to execute JavaScript, we have created a simple example web page. This web page uses JavaScript to write a table to a &lt;div&gt; element. Here is the source code: &lt;html&gt; &lt;body&gt; &lt;div id=&quot;result&quot;&gt;&lt;/div&gt; &lt;script&gt; document.getElementById(&quot;result&quot;).innerHTML = `&lt;table&gt; &lt;tr&gt; &lt;th&gt;Name&lt;/th&gt; &lt;th&gt;Grade&lt;/th&gt; &lt;th&gt;GPA&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Adams&lt;/td&gt; &lt;td&gt;5&lt;/td&gt; &lt;td&gt;4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Alexander&lt;/td&gt; &lt;td&gt;5&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Aaron&lt;/td&gt; &lt;td&gt;5&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Aws&lt;/td&gt; &lt;td&gt;5&lt;/td&gt; &lt;td&gt;3.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Alan&lt;/td&gt; &lt;td&gt;5&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; `; &lt;/script&gt; &lt;/body&gt; &lt;/html&gt; With the traditional approach of downloading the original HTML and parsing the result, the &lt;div&gt; element will be empty, as follows: from lxml import html import requests global_dynamicUrl = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/default/dynamic&quot; global_dynamicPage = requests.get(global_dynamicUrl) global_dynamicHtml = html.fromstring(global_dynamicPage.text) table_area = global_dynamicHtml.xpath(&#39;//*[@id=&quot;result&quot;]/table&#39;) print(table_area) Here is an initial example with Selenium. Selenium can be installed using pip with the command: pip install selenium. The first step is to create a connection to the web browser that we use. Next is to load a web page in the chosen web browser by executing the JavaScript. The JavaScript is executed because now the &lt;div&gt; element has an object representing a table, and within that object, there are 6 objects representing 6 table entries. from selenium import webdriver driver = webdriver.Chrome(&#39;https://driver/chromedriver.exe&#39;) global_dynamicUrl = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/default/dynamic&quot; driver.get(global_dynamicUrl) table_area = driver.find_element_by_xpath(&#39;//*[@id=&quot;result&quot;]/table&#39;) table_entries = table_area.find_elements_by_tag_name(&quot;tr&quot;) print(len(table_entries)) driver.close() So far, our browser automation can only execute JavaScript and access the resulting HTML. To scrape the resulting HTML will require extending the browser automation to support intensive website interactions with the user. Fortunately, Selenium has an excellent API to select and manipulate the HTML elements, which makes this straightforward. Here is an example implementation that rewrites the previous example that searches all students in Selenium. We will cover Selenium in detail in the following sections. from selenium import webdriver import time import string import pandas as pd driver = webdriver.Chrome(&#39;https://driver/chromedriver.exe&#39;) searchAddress = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/cases/search&quot; driver.get(searchAddress) time.sleep(2) students = list() for letter in string.ascii_uppercase: for grade in range(2,8): driver.find_element_by_xpath(&#39;//*[@id=&quot;search_name&quot;]&#39;).send_keys(letter) driver.find_element_by_xpath(&#39;//*[@id=&quot;search_grade&quot;]/option[{}]&#39;.format(grade)).click() driver.find_element_by_xpath(&#39;//*[@id=&quot;search&quot;]&#39;).click() time.sleep(5) try: while True: table = driver.find_element_by_xpath(&#39;//*[@id=&quot;results&quot;]/table&#39;) entries = table.find_elements_by_tag_name(&quot;tr&quot;) for i in range(1, len(entries)): student_dict = dict() cols = entries[i].find_elements_by_tag_name(&quot;td&quot;) student_dict[&quot;name&quot;] = cols[0].text student_dict[&quot;grade&quot;] = cols[1].text student_dict[&quot;gpa&quot;] = cols[2].text students.append(student_dict) try: driver.find_element_by_xpath(&#39;//*[@id=&quot;next&quot;]&#39;).click() time.sleep(2) except: break driver.get(searchAddress) time.sleep(2) except: print(&quot;No results for letter {} at grade {}&quot;.format(letter, grade - 2)) driver.get(searchAddress) time.sleep(2) students_df = pd.DataFrame.from_records(students) print(students_df.head(10)) driver.close() 2.3.3 Comparison of approaches The first approach requires an understanding of how the data is loaded dynamically with JavaScript. This means we must be able to interpret the JavaScript code found in View page source. For the example search web page, we were able to easily know how the JavaScript works. However, some websites will be very complex and difficult to understand. With enough effort, any website can be scraped in this way. However, this effort can be avoided by instead using the Python module Selenium, which automates a web browser to execute JavaScript to display a web page and then perform actions on this web page. This approach only requires that we know how Selenium and its APIs work, so that we can control a web browser. You do not need to understand how the backend of a website works. However, there are disadvantages. Automating a web browser adds overhead and so is much slower than just downloading the HTML. Additionally, solutions using a browser driver often require polling the web page to check whether the resulting HTML from an event has occurred yet or waiting a set amount of time to make sure an AJAX event has completed, which is brittle and can easily fail when the network is slow. "],
["when-to-use-a-browser-driver.html", "3 When to Use a Browser Driver 3.1 Dynamic search 3.2 Dynamic link 3.3 Dynamic load", " 3 When to Use a Browser Driver The Selenium browser driver is typically used to scrape data from dynamic websites that use JavaScript (although it can scrape data from static websites too). The use of JavaScript can vary from simple form events to single page apps that download all their content after loading. The consequence of this is that for many web pages the content that is displayed in our web browser is not available in the original HTML. For example, the following use-cases often occur: A result table shows up only after the user clicks the search box. Content following a click on a link is generated instantaneously rather than already being stored on the server before the click. A JavaScript request might trigger a new block of content to load. The following subsections cover these three use-cases in detail. 3.1 Dynamic search Let us look at an example dynamic web page, which is available at Dynamic Search. This example website has a search form that is used to locate students. Let us say we want to find all the students whose name begins with the letter A and who are in the fifth grade: We place the cursor anywhere on this webpage, right-click and select Inspect from the menu to inspect the results. In the Elements window, we find that the results are stored within a &lt;div&gt; element with ID “results”: Let us try to scrape the information from the result table using the lxml module. This use-case is also covered in detail in our Python Web-Scraping Workshop. from lxml import html import requests search_url = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/cases/search&quot; search_page = requests.get(search_url) search_html = html.fromstring(search_page.text) firstEntry_link = search_html.xpath(&#39;//*[@id=&quot;results&quot;]/table&#39;) print(firstEntry_link) The example scraper here has failed to extract results since the xpath() method returns an empty list. Examining the source code of this web page can help us understand why. Let us put the cursor anywhere on this webpage. Right-click and select View page source from the menu to examine the source code. Here, we find that the &lt;div&gt; element with ID “results” is empty. If we scroll down the source code a little, we find that the display of the result table is coded in a JavaScript function called displayResult(jsonresult) in the JavaScript section. This means that the web page has used JavaScript to load the search results dynamically. At this point, we can recognize that page source code is the original HTML we get when we make a request to the server without executing JavaScript code. In contrast, the Elements window has the HTML that has been revised via running the JavaScript section code. 3.2 Dynamic link Let us now examine what a dynamic link is, and how it is different from a static link. When we click on a link, if it is a static link, the content that appears comes from a file that has been stored on the server before the click. If it is a dynamic link, then the content that appears is generated instantaneously after the click by executing a JavaScript function. To see this, let us Inspect the result table in the Elements window. Look at the two links highlighted below. They have a tag name &lt;a&gt; with an attribute href. The first link is a static link, while the second is a dynamic link: Normally, if a link is static, its link address will end with a file name with a file extension. In this case, the link address (https://iqssdss2020.pythonanywhere.com/tutorial/static/views/Adrian.html) ends with the file name Adrian and file extension .html. The link address is formed first starting with a server address (https://iqssdss2020.pythonanywhere.com) and followed by the web application name (/tutorial) and then followed by a hierarchical path of folders to that .html file. On the other hand, if a link is dynamic, we will not find a file extension at the end of its link address. For example, in this case, the link address (https://iqssdss2020.pythonanywhere.com/tutorial/cases/getstudent/Adams) has no file extension at the end. It is formed first beginning at a server address (https://iqssdss2020.pythonanywhere.com) and followed by the web application name (/tutorial) and then followed by the name of the Python script (cases.py) in the web application (/cases), which calls a function (getstudent()) within this script (/getstudent) followed by a parameter value (/Adams) to the function. Suppose that we temporarily disable the JavaScript execution functionality in our browser by changing the browser’s settings. Let us copy the static link address and open it in a new browser. We should be able to see the profile of the chosen student as below: But if we copy the dynamic link address and open it in a new browser, we will only see an empty webpage with page layout as below: This is because the new browser whose JavaScript functionality has been disabled cannot execute JavaScript code to display the profile content. We can also see this difference using the requests module, which we also covered in detail in our Python Web-Scraping Workshop. staticLink_url = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/static/views/Adams.html&quot; staticLink_page = requests.get(staticLink_url) staticLink_html = html.fromstring(staticLink_page.text) html.open_in_browser(staticLink_html, encoding = &#39;UTF-8&#39;) dynamicLink_url = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/cases/getstudent/Adams&quot; dynamicLink_page = requests.get(dynamicLink_url) dynamicLink_html = html.fromstring(dynamicLink_page.text) html.open_in_browser(dynamicLink_html, encoding = &#39;UTF-8&#39;) The requests module cannot execute JavaScript code. As the code above illustrates, it behaves the same as a browser whose JavaScript functionality is disabled. Examining the source code of the chosen student’s profile web page of both the static and dynamic links can help us further understand why. The source code of the profile web page of a static link shows a complete HTML file without a JavaScript section: In the source code of the profile web page of a dynamic link, we find that the &lt;div&gt; element with ID “results” is empty. There is a JavaScript section next to it. We find that in this section there is a JavaScript page load function. This function makes a request to the server for the chosen student’s profile. The server then returns the information to the function. If successful, this function then displays the profile. This means that the web page has used JavaScript to load the chosen student’s profile dynamically. 3.3 Dynamic load With dynamic loading, new content appears only after a JavaScript request for that information is made to the server. There are two major ways in web application design to make a JavaScript request that triggers a new block of content to load. The first way uses pagination, while the second way involves the scroll bar hitting the bottom of a page. Let us first look at pagination. Let’s inspect the web page of dynamic search again. We can see that the page links are stored within a &lt;div&gt; element with ID “pagination”. Here, the href attribute has a value of javascript:void(0). This just means the browser stays at the same position on the same page and does nothing. Once the page links are clicked, the browser will execute a JavaScript function previous() / next() to make another JavaScript request to the server for the information on that page. Then this new information from the the previous or next page will be displayed. In this case, the value of the href attribute is not a URL. So, there is no point in trying to test if this is a static or dynamic link using the requests module. But we can illustrate the dynamic load using the lxml module. The code below tries to scrape the page link information using the lxml module: search_url = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/cases/search&quot; search_page = requests.get(search_url) search_html = html.fromstring(search_page.text) page_link = search_html.xpath(&#39;//*[@id=&quot;next&quot;]&#39;) print(page_link) The scraper here has failed to extract the page links since the xpath() method returns an empty list. Let us examine the page source code to see why. Here, we find that the &lt;div&gt; element with ID “pagination” is empty: If we scroll down the source code to the end, we find that the display of the page links is coded in a JavaScript function displayResult(jsonresult) in the JavaScript section. This means that the web page has used JavaScript to load the page links and insert it at the position of the &lt;div&gt; element with ID “pagination” in the original HTML. We can see the revised HTML after running the JavaScript code in the Elements window: Now let us examine the second way of dynamically loading content –– scrolling down to the bottom of a page. Let’s look at another example web page, which is available at dynamic search load. This webpage is the same as the previous example webpage, except here new content is loaded when the scroll bar reaches the bottom of a page instead of when clicking a link. The code below tries to scrap the information of the result table’s entries using the lxml module: searchLoad_url = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/casesLoad/search&quot; searchLoad_page = requests.get(searchLoad_url) searchLoad_html = html.fromstring(searchLoad_page.text) entries_link = searchLoad_html.xpath(&#39;//*[@id=&quot;resultstable&quot;]/tbody/tr&#39;) print(page_link) The scraper here has failed to extract that information since the xpath() method returns an empty list. Below we see the result table part of the original HTML from the page source code. It is clear that there is no information under the tag name &lt;tbody&gt;. This explains why the xpath() method returns an empty list. Below we see the same section of the revised HTML from the Elements window after it executes the JavaScript code. The webpage runs the JavaScript to insert the first chunk of the students’ information into the empty result table, which has been created statically, before running the JavaScript and then displaying it in the browser. More interestingly, once we open the Elements window of this example webpage, under the &lt;tbody&gt; tag, we find there are 15 table entries (with tag &lt;tr&gt;). We can therefore infer that the initial load has a total of 15 table entries. If we scroll down to the bottom of the webpage, we can see that 9 more entries are appended to the table. If we continue to scroll down to the bottom of the page, nothing changes. This means that there exists a total of 24 table entries, with the new load delivering the last 9 of those entries. Your browser executes the JavaScript code to perform all these actions. If we look at the JavaScript section in the page source code, we will see that a JQuery method $(window).on(\"scroll\", function() defines the scrolling down to the bottom of page and triggers the load of the new content once it is satisfied. And another JQuery method $('#resultstable &gt; tbody:last-child').append(htmlContent) appends the new entries to the result table. "],
["finding-web-elements.html", "4 Finding Web Elements 4.1 Setting up 4.2 Locating web elements 4.3 Demo 4.4 NoSuchElementException", " 4 Finding Web Elements Selenium works by automating browsers to load the website, retrieve the required data, and even take certain actions on the website. Here, we walk through a practical use-case, using Selenium to extract data from a website. 4.1 Setting up For the successful implementation of browser automation using Selenium, the software WebDriver needs to be set up. The name WebDriver is generic — there are versions of WebDriver for all major browsers. We will now go through the steps to set up WebDriver for Google Chrome, which is called ChromeDriver. Other major browsers have similar steps: Install Selenium using a third-party installer such as pip to install it from the command line via this command: pip install selenium. Download the latest stable release of ChromeDriver from this website, selecting the appropriate version for your operating system. Unzip the downloaded chromedriver*.zip file. An application file named chromedriver.exe should appear. It is recommended that we place the .exe file on the main folder containing the codes. Now that we have completed the setup steps, we can proceed. We have created this example page to run our scraper against. We can start by loading the example page. To begin with, we import WebDriver from Selenium and set a path to chromedriver.exe. Selenium does not contain its own web browser; it requires integration with third party browsers to run. The selenium.webdriver is used to implement various browsers, in this case, Google Chrome. The webdriver.Chrome() method is provided with the path of chromedriver.exe so that it creates an object of the selenium.webdriver.chrome.webdriver.WebDriver class, called “driver” in this case, which will now provide access to the various attributes and properties from WebDriver. The exectuable chromedriver.exe will be instantiated at this instance or upon creation of the driver object. The Terminal screen and an empty new window of Google Chrome will now be loaded. from selenium import webdriver driver = webdriver.Chrome(&#39;C:\\\\Users\\\\JLiu\\\\Desktop\\\\Web_Tutorial\\\\chromedriver.exe&#39;) The new window from Google Chrome is then provided with a URL using the get() function from WebDriver. The get() method accepts the URL that is to be loaded on the browser. We provide our example website address as an argument to get(). Then the browser will start loading the URL: form_url = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/form/search&quot; driver.get(form_url) This can be seen in the following screenshot: As we can see above, a notice is displayed just below the address bar with the message “Chrome is being controlled by automated test software”. This message also confirms the successful execution of the selenium.webdriver activity, and it can be provided with additional code to act on or automate the page that has been loaded. Following successful execution of the code, it is recommended that we close and quit the driver to free up system resources. The close() method terminates the loaded browser window. The quit() method ends the WebDriver application. driver.close() driver.quit() 4.2 Locating web elements After the new Google Chrome window is loaded with the URL provided, we can find the elements that we need to act on. We first need to find the selector or locator information for those elements of interest. The easiest way to identify the information is to Inspect pages using developer tools. Place the cursor anywhere on the webpage, right-click to open a pop-up menu, then select the Inspect option. In the Elements window, move the cursor over the DOM structure of the page until it reaches the desired element. We then need to find information such as what HTML tag is used for the element, the defined attribute, and the values for the attributes and the structure of the page. Next, we need to tell Selenium how to find a particular element or set of elements on a web page programmatically and simulate user actions on these elements. We just need to pass the information we identify in the first step to Selenium. Selenium provides various find_element_by methods to find an element based on its attribute/value criteria or selector value that we supply in our script. If a matching element is found, an instance of WebElement is returned or the NoSuchElementException exception is thrown if Selenium is not able to find any element matching the search criteria. Selenium also provides various find_elements_by methods to locate multiple elements. These methods search and return a list of elements that match the supplied values. Here, we will provide an overview of the various find_element_by_* and find_elements_by_* methods, with some examples of their use. find_element_by_id() and find_elements_by_id() methods: Return an element or a set of elements that have matching ID attribute values. The find_elements_by_id() method returns all the elements that have the same ID attribute values. Let’s try finding the search button from the example website. Here is the HTML code for the search button with an ID attribute value defined as search. We can find this code if we Inspect the site and reach this element in its DOM. &lt;input type=&quot;submit&quot; id=&quot;search&quot; value=&quot;Search&quot; name=&quot;q&quot; class=&quot;button&quot; /&gt; Here is an example that uses the find_element_by_id() method to find the search button. We will pass the ID attribute’s value, search, to the find_element_by_id() method: search_button = driver.find_element_by_id(&quot;search&quot;) find_element_by_name() and find_elements_by_name() methods: Return element(s) that have matching name attribute value(s). The find_elements_by_name() method returns all the elements that have the same name attribute values. Using the previous example, we can instead find the search button using its name attribute value instead of the ID attribute value in the following way: search_button = driver.find_element_by_name(&quot;q&quot;) find_element_by_class_name() and find_elements_by_class_name() methods: Return element(s) that have matching class attribute value(s). The find_elements_by_class_name() method returns all the elements that have the identical class name attribute values. Using the previous example, we can instead find the search button using its class attribute value in following way: search_button = driver.find_element_by_class_name(&quot;button&quot;) find_element_by_tag_name() and find_elements_by_tag_name() methods: Find element(s) by their HTML tag name. The example page displays a search form which has several form fields to fill in. Each form field name is implemented using an &lt;th&gt; or table header cell tag inside a &lt;tr&gt; or table row tag as shown in the following HTML code: We will use the find_elements_by_tag_name() method to get all the form field names. In this example, we will first find the table body implemented as &lt;tbody&gt; using the find_element_by_tag_name() method and then get all the &lt;tr&gt; or table row elements by calling the find_elements_by_tag_name() method on the table body object. For each of the first 4 table rows, we then get its form field name using the &lt;th&gt; tag. table = driver.find_element_by_tag_name(&quot;tbody&quot;) entries = table.find_elements_by_tag_name(&quot;tr&quot;) for i in range(4): header = entries[i].find_element_by_tag_name(&quot;th&quot;).text print(header) find_element_by_xpath() and find_elements_by_xpath() methods: Return element(s) that are found by the specified XPath query. XPath is a query language used to search and locate nodes in a XML document. All major web browsers support XPath. Selenium can leverage and use powerful XPath queries to find elements on a web page. One of the advantages of using XPath is when we can’t find a suitable ID, name, or class attribute value for the element. We can use XPath to either find the element in absolute terms or relative to an element that does have an ID or name attribute. We can also use defined attributes other than the ID, name, or class with XPath queries. We can also find elements with the help of a partial check on attribute values using XPath functions such as starts-with(), contains(), and ends-with(). For example, we want to get the second form field name “Grade”. This element is defined as a &lt;th&gt; tag, but does not have the ID, name, or class attributes defined. Also, we cannot use the find_element_by_tag_name() method as there are multiple &lt;tr&gt; and &lt;th&gt; tags defined on the page. In this case, we can use the find_element_by_xpath() method. To find the XPath of this element, we Inspect the example site, in the Elements window, move the cursor over its DOM structure and find the desired element. We then right-click and choose copy XPath from the pop-up menu. We obtain the following XPath of this element: //*[@id=&quot;table&quot;]/tbody/tr[2]/th This XPath indicates that the path to our desired element starts from the root and then proceeds to an element with a unique id (id=\"table\") and then continues until it reaches the desired element. Please note that the index of the XPath always starts with 1 rather than 0, unlike those of built-in Python data structures. We then pass this XPath to the find_element_by_xpath() method as an argument: second_header = driver.find_element_by_xpath(&#39;//*[@id=&quot;table&quot;]/tbody/tr[2]/th&#39;).text We typically use the XPath method when there exists an element with a unique id on the path to the desired element. Otherwise, this method is not reliable. find_element_by_css_selector() and find_elements_by_css_selector() methods: Return element(s) that are found by the specified CSS selector. CSS is a style sheet language used by web designers to describe the look and feel of a HTML document. CSS is used to define various style classes that can be applied to elements for formatting. CSS selectors are used to find HTML elements based on their attributes such as ID, classes, types, attributes, or values and much more to apply the defined CSS rules. Similar to XPath, Selenium can leverage and use CSS selectors to find elements on a web page. In our previous example, in which we wanted to get the search button on the example site, we can use the following selector, where the selector is defined as the element tag along with the class name. This will find an &lt;input&gt; element with the \"btn-default\" class name. We then test it by automating a click on the search button object we found and find if it starts the search successfully. search_button = driver.find_element_by_css_selector(&quot;input.btn-default&quot;) search_button.click() find_element_by_link_text() and find_elements_by_link_text() methods: Find link(s) using the text displayed for the link. The find_elements_by_link_text() method gets all the link elements that have matching link text. For example, we may want to get the privacy policy link displayed on the example site. Here is the HTML code for the privacy policy link implemented as the &lt;a&gt;, or anchor tag, with text \"privacy policy\": This is the &lt;a id=&quot;privacy_policy&quot; href=&quot;/tutorial/static/views/privacy.html&quot;&gt;privacy policy.&lt;/a&gt;&lt;br/&gt; Let’s create a test that locates the privacy policy link using its text and check whether it’s displayed: privacypolicy_link = driver.find_element_by_link_text(&quot;privacy policy.&quot;) privacypolicy_link.click() find_element_by_partial_link_text() and find_elements_by_partial_link_text() methods: Find link(s) using partial text. For example, on the example site, two links are displayed: one is the privacy policy link with \"privacy policy\" as text and the other is the term conditions policy link with \"term conditions policy\" as text. Let us use this method to find these links using the \"policy\" text and check whether we have two of these links available on the page: policy_links = driver.find_elements_by_partial_link_text(&quot;policy&quot;) print(len(policy_links)) 4.3 Demo This section will highlight two use-cases to demonstrate the use of various find_elements_by methods. Most often we want to scrape data from tables or article text. The two demos therefore cover these use-cases. 4.3.1 Scrape tables Let’s examine this example website. This page uses JavaScript to write a table to a &lt;div&gt; element of the page. If we were to scrape this page’s table using traditional methods, we’d only get the loading page, without actually getting the data that we want. Suppose that we want to scrape all cells of this table. The first thing we need to do is to locate the cells. If we Inspect this page, we can see that the table is defined with a &lt;tbody&gt; tag inside a &lt;table&gt; tag. Each table row is defined with a &lt;tr&gt; tag and there are multiple table rows. The first table row is the table header row, each of its fields is defined with a &lt;th&gt; tag or a header cell tag. In each of the other table rows, there are multiple data cells and each data cell is defined with a &lt;td&gt; tag. Next, we have to find each cell using find_elements_by methods and get its data. There is no way to directly scrape the whole table. Given this, the logic naturally is to loop row by row, and in each row, loop cell by cell. So, we need to have a double for loop in our script. Finally, we need to store the scraped data in a nice format, like a .csv file. We can use the Python file operation methods to achieve this. Here’s an implementation: from selenium import webdriver import time driver = webdriver.Chrome(&#39;C:\\\\Users\\\\JLiu\\\\Desktop\\\\Web_Tutorial\\\\chromedriver.exe&#39;) table_url = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/default/dynamic&quot; driver.get(table_url) time.sleep(2) file = open(&#39;C:\\\\Users\\\\JLiu\\\\Desktop\\\\Web_Tutorial\\\\table.csv&#39;, &quot;w&quot;, encoding = &quot;utf-8-sig&quot;) table_body = driver.find_element_by_xpath(&#39;//*[@id=&quot;result&quot;]/table/tbody&#39;) entries = table_body.find_elements_by_tag_name(&#39;tr&#39;) headers = entries[0].find_elements_by_tag_name(&#39;th&#39;) table_header = &#39;&#39; for i in range(len(headers)): header = headers[i].text if i == len(headers) - 1: table_header = table_header + header + &quot;\\n&quot; else: table_header = table_header + header + &quot;,&quot; file.write(table_header) for i in range(1, len(entries)): cols = entries[i].find_elements_by_tag_name(&#39;td&#39;) table_row = &#39;&#39; for j in range(len(cols)): col = cols[j].text if j == len(cols) - 1: table_row = table_row + col + &quot;\\n&quot; else: table_row = table_row + col + &quot;,&quot; file.write(table_row) driver.close() file.close() 4.3.2 Scrape text Let us examine this example website. The article on this page has many subsections, each of which have multiple paragraphs and even bullet points. Suppose that we want to scrape the whole text of the article. One interesting way to do it is to scrape all the subsections separately first and then concatenate them altogether. The advantage of doing it this way is that we can also get each subsection’s text. Let us Inspect this website. Let us move the cursor to the element of its DOM that defines the article content area. Under this &lt;div&gt; element, we can see that subsection headers have tag names all starting with \"h\", paragraphs have a &lt;p&gt; tag name, and bullet points parts have a &lt;ul&gt; tag name. The elements with these tag names are all parallel with one other, rather than embedded in a hierarchical structure. This design dictates that we should not write a loop in our script to access them, for example, to access each paragraph under a subsection. Another point to note is that here we use a Python dictionary to store each subsection’s text. For each key-value pair in this dictionary, the key stores the subsection title, and the value stores its paragraphs of text. So, this is a convenient data structure to use for this use-case. The following program implements our strategy above to scrape the whole text of the article: from selenium import webdriver import time driver = webdriver.Chrome(&quot;C:\\\\Users\\\\JLiu\\\\Desktop\\\\Web_Tutorial\\\\chromedriver.exe&quot;) journalAddress = &quot;https://www.federalregister.gov/documents/2013/09/24/2013-21228/affirmative-action-and-nondiscrimination-obligations-of-contractors-and-subcontractors-regarding&quot; driver.get(journalAddress) time.sleep(2) articleObjects = driver.find_elements_by_xpath(&#39;//div[@id=&quot;fulltext_content_area&quot;]/*&#39;) print(len(articleObjects), &quot;\\n\\n&quot;) articleDictionary = dict() myKey = &quot;&quot; myValue_total = &quot;&quot; for i in range(len(articleObjects)): tagName = articleObjects[i].tag_name if tagName.startswith(&quot;h&quot;): if myKey: articleDictionary[myKey] = myValue_total myKey = &quot;&quot; myValue_total = &quot;&quot; myKey = articleObjects[i].get_attribute(&quot;innerText&quot;) if tagName.startswith(&quot;p&quot;): myValue = articleObjects[i].get_attribute(&quot;innerText&quot;) myValue_total = myValue_total + myValue if tagName.startswith(&quot;ul&quot;): myBullets = articleObjects[i].find_elements_by_tag_name(&#39;li&#39;) for j in range(len(myBullets)): myBullet = myBullets[j].get_attribute(&quot;innerText&quot;) myValue_total = myValue_total + myBullet driver.close() article = &#39;&#39; for key, value in articleDictionary.items(): article = article + key + &#39;\\n\\n&#39; + value + &#39;\\n\\n***************\\n\\n&#39; print(article) 4.4 NoSuchElementException The reason for NoSuchElementException can be any of the following: The Locator Strategy we have adopted doesn’t identify any element in the HTML DOM. The Locator Strategy we have adopted doesn’t uniquely identify the desired element in the HTML DOM and currently finds some other hidden / invisible element. The Locator Strategy we have adopted is unable to identify the desired element as it is not within the browser’s Viewport. The Locator Strategy we have adopted identifies the element but is invisible due to presence of the attribute style=\"display: none;\". The WebElement we are trying to locate is within an &lt;iframe&gt; tag. The WebDriver instance is looking out for the WebElement even before the element is present/visibile within the HTML DOM. The solution to address the NoSuchElementException can be either of the following: When the element we locate does not exist in the DOM, use try-except event handler to avoid the termination of the program: ```python from selenium.common.exceptions import NoSuchElementException try: elem = driver.find_element_by_xpath(&quot;element_xpath&quot;) elem.click() except NoSuchElementException: pass ``` This solution is to address the inconsistency in the DOM among the seemingly same pages. When the page loads, for some reason we may be taken to the bottom of the page, but the element we need to scrape is on the top of the page and thus is out of view. In this situation, we can locate the element in the DOM first, then use the execute_script() method to scroll the element into view: ```python elem = driver.find_element_by_xpath(&quot;element_xpath&quot;) driver.execute_script(&quot;arguments[0].scrollIntoView();&quot;, elem) ``` In case the element has the attribute style=\"display: none;\", remove the attribute through execute_script() method: ```python elem = driver.find_element_by_xpath(&quot;element_xpath&quot;) driver.execute_script(&quot;arguments[0].removeAttribute(&#39;style&#39;)&quot;, elem) elem.send_keys(&quot;text_to_send&quot;) ``` Adopt a Locator Strategy which uniquely identifies the desired WebElement. The preferable method is find_elements_by_id(), since the id attribute uniquely identifies a web element. To check if the element is within an &lt;iframe&gt;, traverse up the HTML to locate the respective &lt;iframe&gt; tag and use the switch_to() method to shift to the desired iframe through any of the following approaches: ```python driver.switch_to.frame(&quot;iframe_name&quot;) driver.switch_to.frame(&quot;iframe_id&quot;) driver.switch_to.frame(1) // 1 represents frame index ``` We can switch back to the main frame by using one of the following methods: ```python driver.switch_to.default_content() driver.switch_to.parent_frame() ``` A better way to switch frames would be to induce `WebDriverWait()` for the availability of the intended frame with `expected_conditions` set to `frame_to_be_available_and_switch_to_it` as in the following examples: ```python WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it(By.ID,&quot;id_of_iframe&quot;)) // through Frame ID WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it(By.NAME,&quot;name_of_iframe&quot;)) // through Frame name WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it(By.XPATH,&quot;xpath_of_iframe&quot;)) // through Frame XPath WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it(By.CSS_SELECTOR,&quot;css_of_iframe&quot;)) // through Frame CSS ``` If the element is not present/visible in the HTML DOM immediately, induce WebDriverWait with expected_conditions set to the proper method as follows: + To wait for `presence_of_element_located`: ```python element = WebDriverWait(driver, 20).until(expected_conditions.presence_of_element_located((By.XPATH, &quot;element_xpath&#39;]&quot;))) ``` + To wait for `visibility_of_element_located`: ```python element = WebDriverWait(driver, 20).until(expected_conditions.visibility_of_element_located((By.CSS_SELECTOR, &quot;element_css&quot;))) ``` + To wait for `element_to_be_clickable`: ```python element = WebDriverWait(driver, 20).until(expected_conditions.element_to_be_clickable((By.LINK_TEXT, &quot;element_link_text&quot;))) ``` "],
["filling-in-web-forms.html", "5 Filling in Web Forms 5.1 Input box 5.2 Check box 5.3 Radio button 5.4 Link 5.5 Dropdown 5.6 Buttons 5.7 Demos 5.8 ElementNotInteractableException", " 5 Filling in Web Forms Web forms are ubiquituous when surfing the internet. A web form comprises web elements such as input boxes, check boxes, radio buttons, links, drop down menus, and submit buttons to collect user data. To process web forms, we need to first find these web elements and then take subsequent actions on them like selecting a value or entering some text. Selenium has an API that helps us to do that. Since we have covered how to find web element(s) using Selenium selectors in the previous chapter, this chapter focuses on accessing forms in Selenium: performing actions on and interacting with the forms. Let us see how different actions can be performed on each type of web field that may be involved in a web form. We use this example login form website to illustrate most of the examples used in this chapter. Below is a screenshot of how this website uses forms: 5.1 Input box The input box is the text box that displays user input. To handle any input box, we must be able to enter information, clear information or get information from the box. To enter text into a textbox we can use the send_keys() method, which would input the user required text from our automation script. The following code enters the student name starting with “A” into a text box whose ID is “search_name”: driver.find_element_by_id(&#39;search_name&#39;).send_keys(&quot;A&quot;) To clear pre-entered text we can use the clear() method. For example: driver.find_element_by_id(&#39;search_name&#39;).clear() If we need to validate some existing text, we can fetch text already in a text box using the get_attribute() method: nameText = driver.find_element_by_id(&#39;search_name&#39;).get_attribute(&quot;value&quot;) print(nameText) 5.2 Check box A check box is a small box that enables us to check or uncheck it. To select or check a value we use the click() method. It simply changes the state from unchecked to checked and vice-versa. For example, the code below clicks on the Accept Privacy Policy checkbox: driver.find_element_by_id(&#39;privacypolicy&#39;).click() Dealing with checkboxes is not always so straightforward. We may need to select a checkbox only when it is not selected already. Or, we may want to deselect a checkbox only when it is already selected. If we are trying to select a checkbox, but we perform a click operation on an already selected checkbox, then the checkbox will be deselected; something we do not want to happen. So, we first need to validate whether the checkbox is selected or not. To get the current state of the checkbox we can use one of two methods: is_selected() or get_attribute(\"checked\"). For example, using is_selected(): privacy_boolean = driver.find_element_by_id(&#39;privacypolicy&#39;).is_selected() print(privacy_boolean) print(type(privacy_boolean)) This will return a boolean value. This means if the checkbox is checked we would get a True value else we will get False value. Alternatively, using get_attribute(\"checked\"): privacy_other = driver.find_element_by_id(&#39;privacypolicy&#39;).get_attribute(&quot;checked&quot;) print(privacy_other) print(type(privacy_other)) This will return True if the checkbox is selected, but will return NoneType if the checkbox is not selected. The following code shows how to deselect a checkbox only when it is selected: driver.find_element_by_id(&#39;privacypolicy&#39;).click() 5.3 Radio button A radio button is a circular element on the screen that we can select. A radio button is similar to a checkbox, but it is only possible to select one radio button out of several choices, while we can select multiple checkboxes. The actions performed on the radio button are similar to those on a checkbox and we can use the same methods as above to select a radio button or validate its status of selection: click() and is_selected() / get_attribute(“checked”). The code below provides an example: driver.find_element_by_id(&#39;p5&#39;).click() pageSize_5_boolean = driver.find_element_by_id(&#39;p5&#39;).is_selected() print(pageSize_5_boolean) print(type(pageSize_5_boolean)) pageSize_5_other = driver.find_element_by_id(&#39;p5&#39;).get_attribute(&quot;checked&quot;) print(pageSize_5_other) print(type(pageSize_5_other)) Radio buttons do not support deselection. To deselect a radio button, one needs to select any other radio button in that group. If we select the same radio button trying to deselect it, we will get the same selection as before; nothing will change. The following code shows how to deselect a radio button only when it is selected: driver.find_element_by_id(&#39;p10&#39;).click() 5.4 Link A link redirects us to a new web page or a new pop-up window or a similar thing. We can use two mechanisms to navigate to a new screen or a new pop up or a new form: we can either do a click action on a link element we find, or get the new URL from the link element we find and then navigate to it. Here is an example of operating the link embedded in the example webpage using the first mechanism: driver.find_element_by_id(&quot;privacy_policy&quot;).click() Links are generally embedded in the link element we find with a &lt;a&gt; tag name as the \"href\" property. Instead of directly clicking on the link, we can use the get_attribute() method. Here is the same example using the second mechanism: privacy_object = driver.find_element_by_id(&quot;privacy_policy&quot;) privacy_link = privacy_object.get_attribute(&quot;href&quot;) driver.get(privacy_link) 5.5 Dropdown A dropdown is a list which has an arrow at the rightmost end to expand and show values. It provides a list of options to the user, thereby giving access to one or multiple values as per the requirement. To work with a dropdown, first we need to select or find the main element group and then go inside further and select the sub-element that we want to capture. The Selenium Python API provides the Select class, which allows us to select the element of our choice. Note that the Select class only works with tags that have &lt;select&gt; tags. We can select a sub-element of the dropdown using: 1) index, 2) value, or 3) text. If the dropdown has an \"index\" attribute, then we can use that index to select a particular option. We need to be careful when using this approach, because it is not uncommon to have the index start at 0. We can use the select_by_index() method to select an option using the \"index\" attribute. For example, we want to select the 5th grade students: grade_dropdown = Select(driver.find_element_by_id(&quot;search_grade&quot;)) grade_dropdown.select_by_index(6) If the HTML mark-up defines an &lt;option&gt; tag, then we can use the value matching the argument. Suppose the HTML for dropdown is like this: &lt;td&gt; &lt;select id=&quot;search_grade&quot;&gt; &lt;option selected&gt;(no value)&lt;/option&gt; &lt;option value=&quot;K&quot;&gt;K&lt;/option&gt; &lt;option value=&quot;1&quot;&gt;1&lt;/option&gt; &lt;option value=&quot;2&quot;&gt;2&lt;/option&gt; &lt;option value=&quot;3&quot;&gt;3&lt;/option&gt; &lt;option value=&quot;4&quot;&gt;4&lt;/option&gt; &lt;option value=&quot;5&quot;&gt;5&lt;/option&gt; &lt;/select&gt; &lt;/td&gt; We can use the select_by_value() method to select an option using the \"value\" attribute. grade_dropdown.select_by_value(&quot;5&quot;) Probably the easiest way of selecting a sub-element is to select an element using the text of the dropdown. We have to match the text that is displayed in the dropdown using the select_by_visible_text() method. For example: grade_dropdown.select_by_visible_text(&quot;5&quot;) In a similar way, we can deselect any selected value from the dropdown using any of the following options: 1) deselect_by_index(), 2) deselect_by_value(), or 3) deselect_by_visible_text(). These methods can be used only when we make multiple selections. The deselect_all() method clears all the selected options. This is also only applicable when there are multiple selections. If we try to use this when there is a single selection, it will throw a NotImplementedError exception. There are times while performing Selenium automation of our web app, where we need to validate the options coming in our dropdown list. The Select class provides property methods that allow us to do this. The first two property methods are applicable when we can select multiple options. .all_selected_options — Get the list of all the selected options. .first_selected_option — Return the first option that has been selected from the dropdown and unlike the above method it would return a single web element, not a list. .is_multiple — Return True if the dropdown allows multi-selection and return NoneType otherwise. .options — Get a list of all available options in a dropdown. 5.6 Buttons Buttons are simply used to submit whatever information we have filled in our forms to the server. This can be done through click actions on the button, mostly using the click() method: driver.find_element_by_id(&quot;search&quot;).click() 5.7 Demos Using our example site, we will examine a simple program that handles all the types of form elements we have covered so far. Second, we will examine an advanced program that handles the situation where we will need to access this form not once but many times sequentially. 5.7.1 Fill form once Suppose that we want to search all the 5th grade students whose names start with “A” and page size set at 5. The program below demonstrates how to fill in the form and submit it to the server: from selenium import webdriver from selenium.webdriver.support.select import Select driver = webdriver.Chrome(&#39;C:\\\\Users\\\\JLiu\\\\Desktop\\\\Web_Tutorial\\\\chromedriver.exe&#39;) form_url = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/form/search&quot; driver.get(form_url) driver.find_element_by_id(&#39;search_name&#39;).send_keys(&quot;A&quot;) Select(driver.find_element_by_id(&quot;search_grade&quot;)).select_by_visible_text(&quot;5&quot;) driver.find_element_by_id(&#39;p5&#39;).click() driver.find_element_by_id(&quot;privacypolicy&quot;).click() driver.find_element_by_id(&quot;termsconditions&quot;).click() driver.find_element_by_id(&quot;search&quot;).click() 5.7.2 Fill form many times Now suppose that we want to search all the students. This will require us to fill in the form many times, each time with changing input. We have to first play with the website to see if the form webpage will be refreshed every time we access it. This will determine how we are going to write our code. If the form page is refreshed every time we access it, then we have to refill all the form fields each time even if most of those fields are repetitive inputs. The program below shows an example of this use-case: from selenium import webdriver import time import string import pandas as pd driver = webdriver.Chrome(&#39;C:\\\\Users\\\\JLiu\\\\Desktop\\\\Web_Tutorial\\\\chromedriver.exe&#39;) searchAddress = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/form/search&quot; driver.get(searchAddress) time.sleep(2) students = list() for letter in string.ascii_uppercase: for grade in range(2,8): driver.find_element_by_xpath(&#39;//*[@id=&quot;search_name&quot;]&#39;).send_keys(letter) driver.find_element_by_xpath(&#39;//*[@id=&quot;search_grade&quot;]/option[{}]&#39;.format(grade)).click() driver.find_element_by_id(&#39;p5&#39;).click() driver.find_element_by_id(&quot;privacypolicy&quot;).click() driver.find_element_by_id(&quot;termsconditions&quot;).click() driver.find_element_by_xpath(&#39;//*[@id=&quot;search&quot;]&#39;).click() time.sleep(5) try: while True: table = driver.find_element_by_xpath(&#39;//*[@id=&quot;results&quot;]/table&#39;) entries = table.find_elements_by_tag_name(&quot;tr&quot;) for i in range(1, len(entries)): student_dict = dict() cols = entries[i].find_elements_by_tag_name(&quot;td&quot;) student_dict[&quot;name&quot;] = cols[0].text student_dict[&quot;grade&quot;] = cols[1].text student_dict[&quot;gpa&quot;] = cols[2].text students.append(student_dict) try: driver.find_element_by_xpath(&#39;//*[@id=&quot;next&quot;]&#39;).click() time.sleep(2) except: break driver.get(searchAddress) time.sleep(2) except: print(&quot;No results for letter {} at grade {}&quot;.format(letter, grade - 2)) driver.get(searchAddress) time.sleep(2) students_df = pd.DataFrame.from_records(students) print(students_df) driver.close() Line 39 and 43 of this program (driver.get(searchAddress)) indicate that the form will be refreshed whenever we finish the current search. This could happen either when we find out there is no result for this search or when we have scraped the results for this search. Since the form is refreshed every time for a new search, we need to refill the form every time when a new search starts. This is why lines 15 to 19 of the code that execute the form filling actions are put in the most inner layer of the for loop. To know what we need to input for a new search, we have to find out where the search has gone so far up to this point when the form is refreshed for a new search. We control the moving of rounds of search by using the indexing of a list. We put all the options for a form field into a created list with the exact order of the list elements as that being displayed in the form field. When we loop through this list, we control where the search runs through. In the above example, line 13 and 14 play this role. They control the indexing of the name list and the grade list. The name field and the grade field are the only two fields whose input values will change for a new search. This is why we need to create a list for them and then loop over it rather than doing the same thing for all the other form fields. Another scenario is that the form page is not refreshed for a new search. In this case, then we do not need to refill the form fields that will not change their values in a new search. Below is the program that performs the same task –– search all students –– but in the use-case where the form is not refreshed: from selenium import webdriver import time import string import pandas as pd driver = webdriver.Chrome(&#39;C:\\\\Users\\\\JLiu\\\\Desktop\\\\Web_Tutorial\\\\chromedriver.exe&#39;) searchAddress = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/form/search&quot; driver.get(searchAddress) time.sleep(2) students = list() driver.find_element_by_id(&#39;p5&#39;).click() driver.find_element_by_id(&quot;privacypolicy&quot;).click() driver.find_element_by_id(&quot;termsconditions&quot;).click() for letter in string.ascii_uppercase: driver.find_element_by_xpath(&#39;//*[@id=&quot;search_name&quot;]&#39;).clear() driver.find_element_by_xpath(&#39;//*[@id=&quot;search_name&quot;]&#39;).send_keys(letter) for grade in range(2,8): driver.find_element_by_xpath(&#39;//*[@id=&quot;search_grade&quot;]/option[{}]&#39;.format(grade)).click() driver.find_element_by_xpath(&#39;//*[@id=&quot;search&quot;]&#39;).click() time.sleep(5) try: while True: table = driver.find_element_by_xpath(&#39;//*[@id=&quot;results&quot;]/table&#39;) entries = table.find_elements_by_tag_name(&quot;tr&quot;) for i in range(1, len(entries)): student_dict = dict() cols = entries[i].find_elements_by_tag_name(&quot;td&quot;) student_dict[&quot;name&quot;] = cols[0].text student_dict[&quot;grade&quot;] = cols[1].text student_dict[&quot;gpa&quot;] = cols[2].text students.append(student_dict) try: driver.find_element_by_xpath(&#39;//*[@id=&quot;next&quot;]&#39;).click() time.sleep(2) except: break except: print(&quot;No results for letter {} at grade {}&quot;.format(letter, grade - 2)) students_df = pd.DataFrame.from_records(students) print(students_df) driver.close() There is no code in this program that refreshes the form page. We put the lines of code that fill in the constant form fields outside of the loop so that those constant fields will not be refilled in every time for a new search. The change of the positions of lines 13 –- 15 and line 19 of the code comparing the program in the first scenario reflects this idea. 5.8 ElementNotInteractableException In some cases when the element is not interactable, actions on it as introduced in the above sections do not work and we are likely to encounter an ElementNotInteractableException. This is caused when an element is found, but we cannot interact with it — for example, we may not be able to click or send keys. There could be several reasons for this scenario: The element is not visible / not displayed. The element is off screen. The element is behind another element or hidden. Some other action needs to be performed by the user first to enable the element. There are strategies that may work to make the element interactable, depending on the circumstance. 5.8.1 Wait until clickable If the element has not been fully downloaded yet, we can wait until the element is visible / clickable. Look at the following example in which we want to get the profile for the 5th grade student named “Adams”. from selenium import webdriver from selenium.webdriver.support.select import Select driver = webdriver.Chrome(&#39;C:\\\\Users\\\\JLiu\\\\Desktop\\\\Web_Tutorial\\\\chromedriver.exe&#39;) form_url = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/form/search&quot; driver.get(form_url) driver.find_element_by_id(&#39;search_name&#39;).send_keys(&quot;A&quot;) Select(driver.find_element_by_id(&quot;search_grade&quot;)).select_by_visible_text(&quot;5&quot;) driver.find_element_by_id(&#39;p5&#39;).click() driver.find_element_by_id(&quot;privacypolicy&quot;).click() driver.find_element_by_id(&quot;termsconditions&quot;).click() driver.find_element_by_id(&quot;search&quot;).click() table = driver.find_element_by_xpath(&#39;//*[@id=&quot;results&quot;]/table&#39;) entries = table.find_elements_by_tag_name(&quot;tr&quot;) fields = entries[1].find_elements_by_tag_name(&#39;td&#39;) fields[3].find_element_by_tag_name(&quot;a&quot;).click() The above code produces an error message — \"no such element: Unable to locate element\" — because the result table has not been fully downloaded yet. Selenium WebDriver provides two types of waits to handle it –– explicit and implicit wait. The time.sleep() method is an explicit wait to set the condition to be an exact time period to wait, as the code below shows: from selenium import webdriver from selenium.webdriver.support.select import Select import time driver = webdriver.Chrome(&#39;C:\\\\Users\\\\JLiu\\\\Desktop\\\\Web_Tutorial\\\\chromedriver.exe&#39;) form_url = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/form/search&quot; driver.get(form_url) driver.find_element_by_id(&#39;search_name&#39;).send_keys(&quot;A&quot;) Select(driver.find_element_by_id(&quot;search_grade&quot;)).select_by_visible_text(&quot;5&quot;) driver.find_element_by_id(&#39;p5&#39;).click() driver.find_element_by_id(&quot;privacypolicy&quot;).click() driver.find_element_by_id(&quot;termsconditions&quot;).click() driver.find_element_by_id(&quot;search&quot;).click() time.sleep(3) table = driver.find_element_by_xpath(&#39;//*[@id=&quot;results&quot;]/table&#39;) entries = table.find_elements_by_tag_name(&quot;tr&quot;) fields = entries[1].find_elements_by_tag_name(&#39;td&#39;) fields[3].find_element_by_tag_name(&quot;a&quot;).click() As discussed in the previous chapter, a more efficient solution would be to make WebDriver wait only as long as required. This is also an explicit wait but more efficient than time.sleep(). The code below uses the presence of the resulting table element with id \"resulttable\" to declare that the page has been fully loaded: from selenium import webdriver from selenium.webdriver.support.select import Select from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC driver = webdriver.Chrome(&#39;C:\\\\Users\\\\JLiu\\\\Desktop\\\\Web_Tutorial\\\\chromedriver.exe&#39;) form_url = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/form/search&quot; driver.get(form_url) driver.find_element_by_id(&#39;search_name&#39;).send_keys(&quot;A&quot;) Select(driver.find_element_by_id(&quot;search_grade&quot;)).select_by_visible_text(&quot;5&quot;) driver.find_element_by_id(&#39;p5&#39;).click() driver.find_element_by_id(&quot;privacypolicy&quot;).click() driver.find_element_by_id(&quot;termsconditions&quot;).click() driver.find_element_by_id(&quot;search&quot;).click() table = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, &quot;resulttable&quot;))) entries = table.find_elements_by_tag_name(&quot;tr&quot;) fields = entries[1].find_elements_by_tag_name(&#39;td&#39;) fields[3].find_element_by_tag_name(&quot;a&quot;).click() The final solution is to use an implicit wait, which tells WebDriver to poll the DOM for a certain amount of time when trying to find any element(s) not immediately available. The default setting is 0. Once set, the implicit wait is set for the life of the WebDriver object. from selenium import webdriver from selenium.webdriver.support.select import Select driver = webdriver.Chrome(&#39;C:\\\\Users\\\\JLiu\\\\Desktop\\\\Web_Tutorial\\\\chromedriver.exe&#39;) driver.implicitly_wait(10) form_url = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/form/search&quot; driver.get(form_url) driver.find_element_by_id(&#39;search_name&#39;).send_keys(&quot;A&quot;) Select(driver.find_element_by_id(&quot;search_grade&quot;)).select_by_visible_text(&quot;5&quot;) driver.find_element_by_id(&#39;p5&#39;).click() driver.find_element_by_id(&quot;privacypolicy&quot;).click() driver.find_element_by_id(&quot;termsconditions&quot;).click() driver.find_element_by_id(&quot;search&quot;).click() table = driver.find_element_by_xpath(&#39;//*[@id=&quot;results&quot;]/table&#39;) entries = table.find_elements_by_tag_name(&quot;tr&quot;) fields = entries[1].find_elements_by_tag_name(&#39;td&#39;) fields[3].find_element_by_tag_name(&quot;a&quot;).click() 5.8.2 Scroll until on-screen When an element is on screen it means that it is embedded in the DOM structure of the web page. Therefore, we can find its DOM structure when inspecting a web page. If the element we act upon is not in the DOM structure, then we will see some kind of error message. Let us look at this example website. Suppose that we search all the 5th grade students whose names start with “A”. If we Inspect the HTML, we can find that there are 15 table entries included under the &lt;tbody&gt; tag in its DOM structure, as shown in the following figure: Suppose that we want to get the profile for student “Aiden” –– the 15th record of the page. It will work fine. But if we want to get the profile for the 16th student of the page, we will get an error message saying \"list index out of range\". This is because that record is off-screen. from selenium import webdriver from selenium.webdriver.support.select import Select import time driver = webdriver.Chrome(&#39;C:\\\\Users\\\\JLiu\\\\Desktop\\\\Web_Tutorial\\\\chromedriver.exe&#39;) form_url = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/casesLoad/search&quot; driver.get(form_url) driver.find_element_by_id(&#39;search_name&#39;).send_keys(&quot;A&quot;) Select(driver.find_element_by_id(&quot;search_grade&quot;)).select_by_visible_text(&quot;5&quot;) driver.find_element_by_id(&quot;search&quot;).click() time.sleep(5) table = driver.find_element_by_xpath(&#39;//*[@id=&quot;results&quot;]/table&#39;) table_body = table.find_element_by_tag_name(&quot;tbody&quot;) entries = table_body.find_elements_by_tag_name(&quot;tr&quot;) fields = entries[15].find_elements_by_tag_name(&#39;td&#39;) fields[3].find_element_by_tag_name(&#39;a&#39;).click() The solution is to scroll down the page so that the off-screen records can be added into its DOM structure. In the following code, we scroll down to a certain height of the page, where “5000” is the height (line 12). By uncommenting line 14 and commenting line 12, we can also scroll down to the bottom of the page: from selenium import webdriver from selenium.webdriver.support.select import Select import time driver = webdriver.Chrome(&#39;C:\\\\Users\\\\JLiu\\\\Desktop\\\\Web_Tutorial\\\\chromedriver.exe&#39;) form_url = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/casesLoad/search&quot; driver.get(form_url) driver.find_element_by_id(&#39;search_name&#39;).send_keys(&quot;A&quot;) Select(driver.find_element_by_id(&quot;search_grade&quot;)).select_by_visible_text(&quot;5&quot;) driver.find_element_by_id(&quot;search&quot;).click() time.sleep(5) # scroll to a certain height driver.execute_script(&quot;window.scrollTo(0, 5000)&quot;) # scroll to the bottom #driver.execute_script(&quot;window.scrollTo(0, document.body.scrollHeight);&quot;) time.sleep(5) table = driver.find_element_by_xpath(&#39;//*[@id=&quot;results&quot;]/table&#39;) table_body = table.find_element_by_tag_name(&quot;tbody&quot;) entries = table_body.find_elements_by_tag_name(&quot;tr&quot;) fields = entries[15].find_elements_by_tag_name(&#39;td&#39;) fields[3].find_element_by_tag_name(&#39;a&#39;).click() 5.8.3 Execute JavaScript On this example website, we can see that the form submit button is hidden. If we still use the button click() method, we will get an error message saying that \"element not interactable\". In this case, we can opt to execute JavaScript that interacts directly with the DOM: from selenium import webdriver from selenium.webdriver.support.select import Select driver = webdriver.Chrome(&#39;C:\\\\Users\\\\JLiu\\\\Desktop\\\\Web_Tutorial\\\\chromedriver.exe&#39;) driver.implicitly_wait(10) form_url = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/formhidden/search&quot; driver.get(form_url) driver.find_element_by_id(&#39;search_name&#39;).send_keys(&quot;A&quot;) Select(driver.find_element_by_id(&quot;search_grade&quot;)).select_by_visible_text(&quot;5&quot;) driver.find_element_by_id(&#39;p5&#39;).click() driver.find_element_by_id(&quot;privacypolicy&quot;).click() driver.find_element_by_id(&quot;termsconditions&quot;).click() driver.execute_script(f&#39;document.getElementById(&quot;search&quot;).click();&#39;) table = driver.find_element_by_xpath(&#39;//*[@id=&quot;results&quot;]/table&#39;) entries = table.find_elements_by_tag_name(&quot;tr&quot;) fields = entries[1].find_elements_by_tag_name(&#39;td&#39;) fields[3].find_element_by_tag_name(&#39;a&#39;).click() 5.8.4 Perform preliminary action(s) Let us again search for all the 5th grade students whose name starts with “A” on this example website. If we move our cursor over a student name, we will see a hover box showing up above the name. Suppose that we want to scrape the data on the hover box. The hover box is not actionable unless we mover the cursor to a student name to enable it. Once we do that, if we inspect the webpage, we will see the hover box element has been added to its DOM structure. So, we can scrape the data on the hover box from there. In the following code segment we scrape the content on the hover box of student “Adams”: from selenium import webdriver from selenium.webdriver.support.select import Select from selenium.webdriver.common.action_chains import ActionChains driver = webdriver.Chrome(&#39;C:\\\\Users\\\\JLiu\\\\Desktop\\\\Web_Tutorial\\\\chromedriver.exe&#39;) driver.implicitly_wait(10) form_url = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/form/search&quot; driver.get(form_url) driver.find_element_by_id(&#39;search_name&#39;).send_keys(&quot;A&quot;) Select(driver.find_element_by_id(&quot;search_grade&quot;)).select_by_visible_text(&quot;5&quot;) driver.find_element_by_id(&#39;p5&#39;).click() driver.find_element_by_id(&quot;privacypolicy&quot;).click() driver.find_element_by_id(&quot;termsconditions&quot;).click() driver.find_element_by_id(&quot;search&quot;).click() table = driver.find_element_by_xpath(&#39;//*[@id=&quot;results&quot;]/table&#39;) entries = table.find_elements_by_tag_name(&quot;tr&quot;) fields = entries[1].find_elements_by_tag_name(&#39;td&#39;) name_tag = fields[0].find_element_by_tag_name(&quot;span&quot;) hov = ActionChains(driver).move_to_element(name_tag) hov.perform() hov_id = name_tag.get_attribute(&quot;aria-describedby&quot;) print(hov_id) hov_text = driver.find_element_by_id(hov_id).text print(hov_text) We first need to import the ActionChains class in Selenium WebDriver. We create an ActionChains object by passing the driver object. We then find the student name “Adams” object in the page and move the cursor on this object using the method move_to_element(). We then use the method perform() to execute the actions that we have built on the ActionChains object. In this case, this action makes the hover box appear above the student name. Once this is done, the hover box element is added to the DOM structure of the page. By inspecting this new addition in the DOM structure, we can find the ID of the hover box through attribute \"aria-describedby\" and therefore scrape the content of the hover box object associated with that ID. "]
]

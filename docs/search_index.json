[
["index.html", "Template Introduction Table of Contents Authors and Sources", " Template June 2020 Introduction Table of Contents Here, we outline how the guide is organized into parts. First, we… Second, we… Lastly, we… Here we provide an outside link to important content which puts some useful information for this tutorial/workshop at our fingertips. Here we specify where people can provide feedback! Please email help@iq.harvard.edu Authors and Sources Here we acknowledge a few people who helped make this tutorial/workshop possible. We also reference any sources that material was taken from. "],
["concepts.html", "1 Concepts 1.1 How does the web work? 1.2 Uniform Resource Locator (URL) 1.3 Document Object Model (DOM) 1.4 Decision Tree to Choose Web Scraping Approaches", " 1 Concepts We start with the basic concepts that are generic to all web scraping approaches. 1.1 How does the web work? 1.1.1 Components Computers connected to the web are called clients and servers. A simplified diagram of how they interact might look like this: Clients are the typical web user’s internet-connected devices (for example, your computer connected to your Wi-Fi) and web-accessing software available on those devices (usually a web browser like Firefox or Chrome). Servers are computers that store webpages, sites, or apps. When a client device wants to access a webpage, a copy of the webpage is downloaded from the server onto the client machine to be displayed in the user’s web browser. HTTP is a language for clients and servers to speak to each other. 1.1.2 So what happens? When you type a web address into your browser: The browser finds the address of the server that the website lives on. The browser sends an HTTP request message to the server, asking it to send a copy of the website to the client. If the server approves the client’s request, the server sends the client a 200 OK message, and then starts displaying the website in the browser. 1.2 Uniform Resource Locator (URL) To retrieve information from the website (i.e., make a request), we need to know the location of the information we want to collect. The Uniform Resource Locator (URL) — commonly know as a “web address”, specifies the location of a resource (such as a web page) on the internet. A URL is usually composed of 5 parts: The 4th part, the “query string”, contains one or more parameters. The 5th part, the “fragment”, is an internal page reference and may not be present. For example, the URL we want to retrieve data from has the following structure: protocol domain path parameters https www.harvardartmuseums.org browse load_amount=10&amp;offset=0 It is often convenient to create variables containing the domain(s) and path(s) you’ll be working with, as this allows you to swap out paths and parameters as needed. Note that the path is separated from the domain with / and the parameters are separated from the path with ?. If there are multiple parameters they are separated from each other with a &amp;. 1.3 Document Object Model (DOM) To parse HTML, we need to have a nice tree structure that contains the whole HTML file through which we can locate the information. This tree-like structure is the Document Object Model (DOM). DOM is a cross-platform and language-independent interface that treats an XML or HTML document as a tree structure wherein each node is an object representing a part of the document. The DOM represents a document with a logical tree. Each branch of the tree ends in a node, and each node contains objects. DOM methods allow programmatic access to the tree; with them one can change the structure, style or content of a document. The following is an example of DOM hierarchy in an HTML document: 1.4 Decision Tree to Choose Web Scraping Approaches There are many commonly used web scraping approaches. The decision tree as follows will help you to decide upon the best approach to use for a particular web site. "],
["approaches-to-web-scraping.html", "2 Approaches to Web Scraping 2.1 Approaches to Scraping a Static Web Page 2.2 Approaches to Scraping a Dynamic Web Page", " 2 Approaches to Web Scraping If the content you are viewing in your browser does not match the content you see in the HTML source code you are retrieving from the site, then you are experiencing the dynamic websites. Otherwise, if they match with each other, the websites are static. The mismatch is due to the execution of JavaScript that changes the HTML elements on the page. You could view the original HTML via View page source. You could view the revised HTML in your browser if it executes JavaScript in the Elements window via Inspecting the web page. 2.1 Approaches to Scraping a Static Web Page There are three approaches to extracting data from a static webpage that has been downloaded: using regular expressions, using Beautiful Soup module, and finally using lxml module. We use this static student profile webpage to provide examples for each approach. Suppose that we want to scrape the student name. The data we are interested in is found in this part of the HTML. The student name is included within a &lt;td&gt; element of class w2p_fw, which is the child of a &lt;tr&gt; element of ID students_name_row. &lt;table&gt; &lt;tr id=&quot;students_name_row&quot;&gt;&lt;td class=&quot;w2p_fl&quot;&gt;&lt;label for=&quot;students_name&quot; id=&quot;students_name_label&quot;&gt;Name:&lt;/label&gt;&lt;/td&gt;&lt;td class=&quot;w2p_fw&quot;&gt;Adams&lt;/td&gt; &lt;td class=&quot;w2p_fc&quot;&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr id=&quot;students_school_row&quot;&gt;&lt;td class=&quot;w2p_fl&quot;&gt;&lt;label for=&quot;students_school&quot; id=&quot;students_school_label&quot;&gt;School:&lt;/label&gt;&lt;/td&gt;&lt;td class=&quot;w2p_fw&quot;&gt;IV&lt;/td&gt; &lt;td class=&quot;w2p_fc&quot;&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr id=&quot;students_level_row&quot;&gt;&lt;td class=&quot;w2p_fl&quot;&gt;&lt;label for=&quot;students_level&quot; id=&quot;students_level_label&quot;&gt;Advanced:&lt;/label&gt;&lt;/td&gt;&lt;td class=&quot;w2p_fw&quot;&gt;No&lt;/td&gt; &lt;td class=&quot;w2p_fc&quot;&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; 2.1.1 Regular Expressions Regular expressions directly work on a downloaded web page with no need of parsing it into a certain format and try to match the content of the part of the HTML that contains the data you want to scrape from. There is a thorough overview of regular expressions here. In this example, we need to match the &lt;td class=\"w2p_fw\"&gt; tag to scrape the student name. But this tag is used for multiple student profile attributes. To isolate the name, we select the first element, as shown in the code below. import re import requests url = &#39;https://iqssdss2020.pythonanywhere.com/tutorial/static/views/Adams.html&#39; html = requests.get(url) mylist = re.findall(&#39;&lt;td class=&quot;w2p_fw&quot;&gt;(.*?)&lt;/td&gt;&#39;, html.text) print(mylist) ## [&#39;Adams&#39;, &#39;IV&#39;, &#39;No&#39;] name = re.findall(&#39;&lt;td class=&quot;w2p_fw&quot;&gt;(.*?)&lt;/td&gt;&#39;, html.text)[0] print(name) ## Adams This solution works now but could easily fail if the web page is updated later. Consider if the student ID data is inserted right before the student name. Then we must change the code to select the second element. The general solution to make a regular expression scraper more robust is to include the parent element, which has an ID, so it ought to be unique: import re import requests url = &#39;https://iqssdss2020.pythonanywhere.com/tutorial/static/views/Adams.html&#39; html = requests.get(url) mylist = re.findall(&#39;&lt;tr id=&quot;students_name_row&quot;&gt;&lt;td class=&quot;w2p_fl&quot;&gt;&lt;label for=&quot;students_name&quot; id=&quot;students_name_label&quot;&gt;Name:\\ &lt;/label&gt;&lt;/td&gt;&lt;td class=&quot;w2p_fw&quot;&gt;(.*?)&lt;/td&gt;&#39;, html.text) print(mylist) ## [&#39;Adams&#39;] This solution is better. However, there are many other ways the web page could be updated that still break the regular expression. For example, double quotation might be changed to single for class name, extra space could be added between the &lt;td&gt; tags, or the name_label could be changed. The general solution to it is to make the regular expression as generic as possible to support various possibilities: import re import requests url = &#39;https://iqssdss2020.pythonanywhere.com/tutorial/static/views/Adams.html&#39; html = requests.get(url) mylist = re.findall(&#39;&lt;tr id=&quot;students_name_row&quot;&gt;.*?&lt;td\\s*class=[&quot;\\&#39;]w2p_fw[&quot;\\&#39;]&gt;(.*?)&lt;/td&gt;&#39;, html.text) print(mylist) ## [&#39;Adams&#39;] This regular expression is more robust to webpage updates but is more difficult to construct, becoming even unreadable. But still, there are other minor layout changes that would break it, such as if a title attribute is added to the &lt;td&gt; tag. From this example, regular expressions provide a quick way to scrape data without the step of parsing but are too brittle and will easily break when a web page is updated. 2.1.2 Beautiful Soup Beautiful Soup is a popular module that parses a downloaded web page into a certain format and then provides a convenient interface to navigate content. The official documentation of Beautiful Soup can be found here. The latest version of the module can be installed using this command: pip install beautifulsoup4. The first step with Beautiful Soup is to parse the downloaded HTML into a soup document. Beautiful Soup supports several different parsers. Parsers behave differently when parsing web pages that do not contain perfectly valid HTML. For example, consider this HTML syntax of a table entry with missing attribute quotes and closing tags for the table row and table fields: &lt;tr id=students_school_row&gt; &lt;td class=w2p_fl&gt; &lt;label for=&quot;students_school&quot; id=&quot;students_school_label&quot;&gt; School: &lt;/label&gt; &lt;td class=w2p_fw&gt;IV Beautiful Soup with the lxml parser can correctly interpret the missing attribute quotes and closing tags, as well as add the &lt;html&gt; and &lt;body&gt; tags to form a complete HTML document, as the code below shows: from bs4 import BeautifulSoup broken_html = &#39;&lt;tr id=students_school_row&gt;&lt;td class=w2p_fl&gt;&lt;label for=&quot;students_school&quot; id=&quot;students_school_label&quot;&gt;School:&lt;/label&gt;&lt;td class=w2p_fw&gt;IV&#39; soup = BeautifulSoup(broken_html, &#39;lxml&#39;) fixed_html = soup.prettify() print(fixed_html) ## &lt;html&gt; ## &lt;body&gt; ## &lt;tr id=&quot;students_school_row&quot;&gt; ## &lt;td class=&quot;w2p_fl&quot;&gt; ## &lt;label for=&quot;students_school&quot; id=&quot;students_school_label&quot;&gt; ## School: ## &lt;/label&gt; ## &lt;/td&gt; ## &lt;td class=&quot;w2p_fw&quot;&gt; ## IV ## &lt;/td&gt; ## &lt;/tr&gt; ## &lt;/body&gt; ## &lt;/html&gt; But if we use the html.parser, it interprets the school name itself as a child of the school key instead of the parallel table fields and it does not create a complete HTML, as the code below shows: from bs4 import BeautifulSoup broken_html = &#39;&lt;tr id=students_school_row&gt;&lt;td class=w2p_fl&gt;&lt;label for=&quot;students_school&quot; id=&quot;students_school_label&quot;&gt;School:&lt;/label&gt;&lt;td class=w2p_fw&gt;IV&#39; soup = BeautifulSoup(broken_html, &#39;html.parser&#39;) fixed_html = soup.prettify() print(fixed_html) ## &lt;tr id=&quot;students_school_row&quot;&gt; ## &lt;td class=&quot;w2p_fl&quot;&gt; ## &lt;label for=&quot;students_school&quot; id=&quot;students_school_label&quot;&gt; ## School: ## &lt;/label&gt; ## &lt;td class=&quot;w2p_fw&quot;&gt; ## IV ## &lt;/td&gt; ## &lt;/td&gt; ## &lt;/tr&gt; However, keep it in mind that none of these parsers is always the correct way to handle invalid HTML. It will be case-by-case. The next step of using Beautiful Soup is to navigate to the elements of HTML we want using its API. Here is an example to extract the student name from our example profile webpage: from bs4 import BeautifulSoup import requests url = &#39;https://iqssdss2020.pythonanywhere.com/tutorial/static/views/Adams.html&#39; html = requests.get(url) soup = BeautifulSoup(html.text, &#39;html.parser&#39;) tr = soup.find(attrs={&#39;id&#39;:&#39;students_name_row&#39;}) td = tr.find(attrs={&#39;class&#39;:&#39;w2p_fw&#39;}) name = td.text print(name) ## Adams This code is longer than regular expressions but easier to construct and understand. Also, we no longer need to worry about problems in minor layout changes, such as extra whitespace or tag attributes. 2.1.3 Lxml The lxml module is a Python wrapper on the top of the C libraries libxml2 and libxslt. It works the same way as Beautiful Soup but is much faster. The documentation of lxml can be found here. The module can be installed using this command: pip install lxml. As with Beautiful Soup, the first step of lxml is parsing the potentially invalid HTML into a consistent format. Here is an example of parsing the same broken HTML: from lxml import etree, html broken_html = &#39;&lt;tr id=students_school_row&gt;&lt;td class=w2p_fl&gt;&lt;label for=&quot;students_school&quot; id=&quot;students_school_label&quot;&gt;School:&lt;/label&gt;&lt;td class=w2p_fw&gt;IV&#39; tree = html.fromstring(broken_html) fixed_html = etree.tostring(tree, pretty_print=True).decode(&#39;utf-8&#39;) print(fixed_html) ## &lt;tr id=&quot;students_school_row&quot;&gt; ## &lt;td class=&quot;w2p_fl&quot;&gt; ## &lt;label for=&quot;students_school&quot; id=&quot;students_school_label&quot;&gt;School:&lt;/label&gt; ## &lt;/td&gt; ## &lt;td class=&quot;w2p_fw&quot;&gt;IV&lt;/td&gt; ## &lt;/tr&gt; As with Beautiful Soup, lxml was able to correctly parse the missing attribute quotes and closing tags, although it did not add the &lt;html&gt; and &lt;body&gt; tags. Here we use lxml.etree module to formulate a more hierarchical tree structure and then convert it to text via tostring() method in order to display it. After parsing the input, lxml has its API to select elements, such as XPath selectors, like Beautiful Soup. Here is an example using the lxml xpath() method to extract the student name data: from lxml import etree, html import requests static_url = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/static/views/Adams.html&quot; static_html = requests.get(static_url) tree = html.fromstring(static_html.text) name = tree.xpath(&#39;//*[@id=&quot;students_name_row&quot;]/td[2]&#39;)[0].text print(name) ## Adams 2.1.4 Comparison of Approaches As shown in the previous sections, Beautiful Soup and lxml are more robust to webpage changes than regular expressions. Comparing their relative efficiency, lxml and the regular expression module were written in C, while Beautiful Soup is pure Python. So, lxml and regular expressions are much faster than Beautiful Soup. We did an experiment that ran each scraper to extract all the available student profile data 1000 times and record the total time taken by each scraper. A full implementation of this experiment can be found as follows, as well as the results from running this script on my computer: import re from bs4 import BeautifulSoup from lxml import html import time import requests fields = [&quot;name&quot;, &quot;school&quot;, &quot;level&quot;] def re_scraper(htmlText): results = {} for field in fields: results[field] = re.findall(&#39;&lt;tr id=&quot;students_{}_row&quot;&gt;.*?&lt;td class=&quot;w2p_fw&quot;&gt;(.*?)&lt;/td&gt;&#39;.format(field), htmlText)[0] return results def bs_scraper(htmlText): soup = BeautifulSoup(htmlText, &#39;html.parser&#39;) results = {} for field in fields: results[field] = soup.find(attrs={&#39;id&#39;:&#39;students_{}_row&#39;.format(field)}).find(attrs={&#39;class&#39;:&#39;w2p_fw&#39;}).text return results def lxml_scraper(htmlText): tree = html.fromstring(htmlText) results = {} for field in fields: results[field] = tree.xpath(&#39;//*[@id=&quot;students_{}_row&quot;]/td[2]&#39;.format(field))[0].text return results num_iterations = 1000 static_html = requests.get(&quot;https://iqssdss2020.pythonanywhere.com/tutorial/static/views/Adams.html&quot;).text for name, scraper in [(&#39;Regular Expressions&#39;, re_scraper), (&#39;Beautiful Soup&#39;, bs_scraper), (&#39;Lxml&#39;, lxml_scraper)]: start = time.time() for i in range(num_iterations): if scraper == re_scraper: re.purge() result = scraper(static_html) assert(result[&quot;name&quot;] == &quot;Adams&quot;) assert(result[&quot;school&quot;] == &quot;IV&quot;) assert(result[&quot;level&quot;] == &quot;No&quot;) end = time.time() print(&#39;{}: {} seconds&#39;.format(name, end - start)) ## Regular Expressions: 0.5155882835388184 seconds ## Beautiful Soup: 1.2968761920928955 seconds ## Lxml: 0.21877646446228027 seconds The results show that Beautiful Soup is much slower than the other two approaches. Regular expressions does not perform the fastest, because we call re.purge() in every iteration to clear cache. By default, the regular expression module will cache searches and this cache needs to be cleared to make a fair comparison with the other scraping approaches. lxml performs comparatively well with regular expressions, although lxml has the additional overhead of having to parse the input into its internal format before searching for elements. When scraping many features from a web page, this initial parsing overhead is reduced and lxml becomes even more competitive. 2.2 Approaches to Scraping a Dynamic Web Page There are two approaches to scraping a dynamic webpage: scrape the content directly from the JavaScript, or use Python packages capable of executing the JavaScript itself, and scrape the website as you view it in your browser. 2.2.1 AJAX Requests Because the data is loaded dynamically with JavaScript, to scrape this data, we need to understand how the web page loads this data. Suppose that we want to find all students whose names start with letter A in the fifth grade with page size set at 5 from this example dynamic web page. After we click Search button, open Fiddler—a software that can inspect HTTP requests on your computer and can be downloaded here. We will see that an AJAX request is made. Under Request Headers in the Inspectors window, we can find the URL for this search. Under Response window, we can see the response content is in JSON format. They are highlighted in blue in the figure as follows: AJAX stands for Asynchronous JavaScript and XML. A dynamic web page works because the AJAX allows JavaScript to make HTTP requests to a remote server and receive responses. This approach is to first access to the AJAX request responses, and then to scrape information of interest from them. The AJAX response data can be downloaded directly. With the URL of the response, we can make a request to the server, scrape the information from the response, and store the scraped information in a spreadsheet, as the following code shows: import requests import pandas as pd html = requests.get(&#39;https://iqssdss2020.pythonanywhere.com/tutorial/cases/search_ajax?search_name=A&amp;search_grade=5&amp;page_size=5&amp;page=1&#39;) html_json = html.json() print(html_json) ## {&#39;records&#39;: [{&#39;Name&#39;: &#39;Annie&#39;, &#39;Grade&#39;: &#39;5&#39;, &#39;GPA&#39;: 3.0}, {&#39;Name&#39;: &#39;Ala&#39;, &#39;Grade&#39;: &#39;5&#39;, &#39;GPA&#39;: 2.5}, {&#39;Name&#39;: &#39;Aayusha&#39;, &#39;Grade&#39;: &#39;5&#39;, &#39;GPA&#39;: 3.5}, {&#39;Name&#39;: &#39;Anushri&#39;, &#39;Grade&#39;: &#39;5&#39;, &#39;GPA&#39;: 4.0}, {&#39;Name&#39;: &#39;Andrew&#39;, &#39;Grade&#39;: &#39;5&#39;, &#39;GPA&#39;: 3.0}], &#39;num_pages&#39;: 5, &#39;error&#39;: &#39;&#39;} students_A5p0 = pd.DataFrame.from_records(html_json[&#39;records&#39;]) print(students_A5p0.head(10)) ## Name Grade GPA ## 0 Annie 5 3.0 ## 1 Ala 5 2.5 ## 2 Aayusha 5 3.5 ## 3 Anushri 5 4.0 ## 4 Andrew 5 3.0 Here is an example implementation that scrapes all the students by searching for each letter of the alphabet and each grade, and then iterating the resulting pages of the JSON responses. The results are then stored in a spreadsheet. import requests import pandas as pd import string temp_url = &#39;https://iqssdss2020.pythonanywhere.com/tutorial/cases/search_ajax?search_name={}&amp;search_grade={}&amp;page_size=5&amp;page={}&#39; students = list() grades = [&quot;K&quot;, &quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;] for letter in string.ascii_uppercase: for grade in grades: page = 0 while True: url = temp_url.format(letter, grade, page) html = requests.get(url) html_json = html.json() students.extend(html_json[&quot;records&quot;]) page += 1 if page &gt;= html_json[&quot;num_pages&quot;]: break students_df = pd.DataFrame.from_records(students) print(students_df.head(10)) ## Name Grade GPA ## 0 Allen 1 3 ## 1 Anderson 4 3.5 ## 2 Adams 5 4 ## 3 Alexander 5 1 ## 4 Aaron 5 3 ## 5 Aws 5 3.5 ## 6 Alan 5 2 ## 7 Annie 5 3 ## 8 Ala 5 2.5 ## 9 Aayusha 5 3.5 The AJAX-dependent websites initially look more complex but their structure encourages separating the data transmission between client and server and the data presentation on the client browser executing JavaScript, which can make our job of extracting this data much easier. 2.2.2 Selenium The second approach uses Python packages capable of executing the JavaScript itself, and scrape the website as you view it in your browser. Selenium works by automating browsers to execute JavaScript to display a web page as we expect. To confirm that Selenium can automate browser to execute JavaScript, this is a simple example web page. This web page simply uses JavaScript to write a table to a &lt;div&gt; element. Here is the source code: &lt;html&gt; &lt;body&gt; &lt;div id=&quot;result&quot;&gt;&lt;/div&gt; &lt;script&gt; document.getElementById(&quot;result&quot;).innerHTML = `&lt;table&gt; &lt;tr&gt; &lt;th&gt;Name&lt;/th&gt; &lt;th&gt;Grade&lt;/th&gt; &lt;th&gt;GPA&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Adams&lt;/td&gt; &lt;td&gt;5&lt;/td&gt; &lt;td&gt;4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Alexander&lt;/td&gt; &lt;td&gt;5&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Aaron&lt;/td&gt; &lt;td&gt;5&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Aws&lt;/td&gt; &lt;td&gt;5&lt;/td&gt; &lt;td&gt;3.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Alan&lt;/td&gt; &lt;td&gt;5&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; `; &lt;/script&gt; &lt;/body&gt; &lt;/html&gt; With the traditional approach of downloading the original HTML and parsing the result, the &lt;div&gt; element will be empty, as follows: from lxml import html import requests global_dynamicUrl = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/default/dynamic&quot; global_dynamicPage = requests.get(global_dynamicUrl) global_dynamicHtml = html.fromstring(global_dynamicPage.text) table_area = global_dynamicHtml.xpath(&#39;//*[@id=&quot;result&quot;]/table&#39;) print(table_area) ## [] Here is an initial example with Selenium. Selenium can be installed using pip with the command: pip install selenium. The first step is to create a connection to the web browser that you use. Next is to load a web page in the chosen web browser via executing the JavaScript. The JavaScript is executed because now the &lt;div&gt; element has an object representing a table, and within that object, there are 6 objects representing 6 table entries. from selenium import webdriver driver = webdriver.Chrome(&#39;https://driver/chromedriver.exe&#39;) global_dynamicUrl = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/default/dynamic&quot; driver.get(global_dynamicUrl) table_area = driver.find_element_by_xpath(&#39;//*[@id=&quot;result&quot;]/table&#39;) table_entries = table_area.find_elements_by_tag_name(&quot;tr&quot;) print(len(table_entries)) driver.close() So far, our browser automaton can only execute JavaScript and access the resulting HTML. To scrape the resulting HTML will require extending the browser automation to support intensive website interactions with the user. Fortunately, Selenium has an excellent API to select and manipulate the HTML elements, which makes this straightforward. Here is an example implementation that rewrites the previous search all students example in Selenium. We will cover Selenium in detail in the following chapters. from selenium import webdriver import time import string import pandas as pd driver = webdriver.Chrome(&#39;https://driver/chromedriver.exe&#39;) searchAddress = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/cases/search&quot; driver.get(searchAddress) time.sleep(2) students = list() for letter in string.ascii_uppercase: for grade in range(2,8): driver.find_element_by_xpath(&#39;//*[@id=&quot;search_name&quot;]&#39;).send_keys(letter) driver.find_element_by_xpath(&#39;//*[@id=&quot;search_grade&quot;]/option[{}]&#39;.format(grade)).click() driver.find_element_by_xpath(&#39;//*[@id=&quot;search&quot;]&#39;).click() time.sleep(5) try: while True: table = driver.find_element_by_xpath(&#39;//*[@id=&quot;results&quot;]/table&#39;) entries = table.find_elements_by_tag_name(&quot;tr&quot;) for i in range(1, len(entries)): student_dict = dict() cols = entries[i].find_elements_by_tag_name(&quot;td&quot;) student_dict[&quot;name&quot;] = cols[0].text student_dict[&quot;grade&quot;] = cols[1].text student_dict[&quot;gpa&quot;] = cols[2].text students.append(student_dict) try: driver.find_element_by_xpath(&#39;//*[@id=&quot;next&quot;]&#39;).click() time.sleep(2) except: break driver.get(searchAddress) time.sleep(2) except: print(&quot;No results for letter {} at grade {}&quot;.format(letter, grade - 2)) driver.get(searchAddress) time.sleep(2) students_df = pd.DataFrame.from_records(students) print(students_df.head(10)) driver.close() 2.2.3 Comparison of Approaches Because the first approach needs to understand how the data is loaded dynamically with JavaScript, it needs you to understand the JavaScript code, which can be found in View page source. For the example search web page, we were able to easily know how it works. However, some websites will be very complex and difficult to understand. With enough effort, any website can be scraped in this way. However, this effort can be avoided by instead using a Python package Selenium that automates a web browser to execute JavaScript to display a web page as we expect and then perform actions on this web page. Doing this way only needs you to know how Selenium works and its APIs that control a web browser. You do not need to understand how the backend of a website works. However, there are disadvantages. Automating a web browser adds overhead and so is much slower than just downloading the HTML. Additionally, solutions using a browser driver often require polling the web page to check whether the resulting HTML from an event has occurred yet or waiting a set amount of time for an AJAX event is complete by then, which is brittle and can easily fail when the network is slow. "],
["when-to-use-selenium-driver.html", "3 When to Use Selenium Driver? 3.1 An Example of Dynamic Search 3.2 An Example of Dynamic Link 3.3 An Example of Dynamic New Load", " 3 When to Use Selenium Driver? The Selenium driver is particularly used to scrape data from dynamic websites that use JavaScript (although it can scrape data from static websites too). The use of JavaScript can vary from simple form events to single page apps that download all their content after loading. The consequence of this is that for many web pages the content that is displayed in our web browser is not available in the original HTML. For example, the result table shows up only after the user click the search box, the content following a click on a link is generated instantaneously rather than already being stored on the server before the click, or a JavaScript request might trigger a new block of content to load. The following subsections of this part cover those three examples in detail. This tutorial will cover the scraping techniques that use Python Selenium package capable of executing the JavaScript itself, and scrape the website as you view it in your browser. 3.1 An Example of Dynamic Search Let us look at an example dynamic web page, which is available at Dynamic Search. This example website has a search form that is used to locate students. Let us say we want to find all the students whose name begins with the letter A and who are in the fifth grade: Figure I-1.1 We place the mouse anywhere on this webpage. We right-click the mouse and click inspect from the menu to inspect the results. In the Elements window, we would find that the results are stored within a &lt;div&gt; element with ID “results”: Figure I-1.2 Let us try to scrape the information from the result table using the lxml module, which was also covered in detail in the Python Web-Scraping Workshop. from lxml import html import requests search_url = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/cases/search&quot; search_page = requests.get(search_url) search_html = html.fromstring(search_page.text) firstEntry_link = search_html.xpath(&#39;//*[@id=&quot;results&quot;]/table&#39;) print(firstEntry_link) ## [] The example scraper here has failed to extract results since the xpath() method returns an empty list. Examining the source code of this web page can help you understand why. Let us put the mouse anywhere on this webpage. Right-click the mouse and click View page source from the menu to examine the source code. Here, we find that the &lt;div&gt; element with ID “results” is empty. Figure I-1.3 If we scroll down the source code a little bit, we can find that the display of the result table is coded in a JavaScript function called as displayResult(jsonresult) in the JavaScript section. This means that the web page has used JavaScript to load the search results dynamically. At this point, you could recognize that page source code is the original HTML you get when you make a request to the server without executing JavaScript code. In contrast, the Elements window has the HTML that has been revised via running the JavaScript section code. Figure I-1.4 3.2 An Example of Dynamic Link Let us now examine what a dynamic link is, and how it is different from a static link. When you click on a link, if it is a static link, the content that appears comes from a file that has been stored on the server before the click. If it is a dynamic link, then the content that appears is generated instantaneously after the click by executing a JavaScript function. Then let us take a further look about how different a dynamic link is from a static link. Let us still inspect the result table in the Elements window. Look at the two links highlighted in Figure I-2.1. These two links have a tag name &lt;a&gt; with an attribute href. The first link is a static link, while the second is a dynamic link. Figure I-2.1 Normally, if a link is a static link, its link address will end with a file name with a file extension. In this case, the link address (https://iqssdss2020.pythonanywhere.com/tutorial/static/views/Adrian.html) ends with a .html file. The link address is formed first starting with a server address (https://iqssdss2020.pythonanywhere.com) and followed by the web application name (/tutorial) and then followed by a hierarchical path of folders to that .html file. Figure I-2.2 On the other hand, if a link is a dynamic link, you will not find a file extension at the end of its link address. For example, in this case, the link address (https://iqssdss2020.pythonanywhere.com/tutorial/cases/getstudent/Adams) has no a file extension at the end. It is formed first beginning at a server address (https://iqssdss2020.pythonanywhere.com) and followed by the web application name (/tutorial) and then followed by the name of the Python script (cases.py) in the web application (/cases) that calls a function in this script called as getstudent() (/getstudent) followed with a parameter value (/Adams) to the getstudent() function. Figure I-2.3 Suppose that we temporarily disable the JavaScript execution functionality in our browser by changing the browser’s settings. Let us copy the static link address and open it in a new browser. We should be able to see the profile of the chosen student as Figure 1-2.4 shows. But if we copy the dynamic link address and open it in a new browser, we will only see an empty webpage with just page layout as Figure 1-2.5 shows. This is because the new browser whose JavaScript functionality has been disabled cannot execute JavaScript code in order to display the profile content. Figure I-2.4 Figure I-2.5 We can also see this difference using the requests module, which was also covered in detail in the Python Web-Scraping Workshop. staticLink_url = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/static/views/Adams.html&quot; staticLink_page = requests.get(staticLink_url) staticLink_html = html.fromstring(staticLink_page.text) html.open_in_browser(staticLink_html, encoding = &#39;UTF-8&#39;) dynamicLink_url = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/cases/getstudent/Adams&quot; dynamicLink_page = requests.get(dynamicLink_url) dynamicLink_html = html.fromstring(dynamicLink_page.text) html.open_in_browser(dynamicLink_html, encoding = &#39;UTF-8&#39;) The requests module cannot execute JavaScript code. As the code above illustrates, it behaves the same as a browser whose JavaScript functionality is disabled. Examining the source code of the chosen student’s profile web page of both the static and dynamic links can help you further understand why. The source code of the profile web page of a static link shows a complete html file without a JavaScript section. Figure I-2.6 In the source code of the profile web page of a dynamic link, we find that the &lt;div&gt; element with ID “results” is empty. There is a JavaScript section next to it. We can find that in this section there is a JavaScript page load function. This function makes a request to the server for the chosen student’s profile. The server then returns the information to the function. If successful, this function then displays the profile. This means that the web page has used JavaScript to load the chosen student’s profile dynamically. Figure I-2.7 3.3 An Example of Dynamic New Load The dynamic load means that the new content appears only after a JavaScript request for that information is made to the server. There are two major ways in web application design to make a JavaScript request in order to trigger a new block of content to load. The first way uses pagination, and the second way scrolls bar to the bottom of a page. Let us first look at pagination. Let us inspect the web page of dynamic search again. We would find that the page links are stored within a &lt;div&gt; element with ID “pagination”. Here, the href attribute has a value of javascript:void(0). It just means the browser stays at the same position on the same page and does nothing. Once the page links are clicked, the browser will execute a JavaScript function previous() / next() to make another JavaScript request to the server for the information on that page and then display those new information coming from the server on the previous or next page via executing the relevant JavaScript functions. Figure I-3.1 In this case, the value of the href attribute is not an URL (The Uniform Resource Locator). So, there is no point to try to test if this is a static or dynamic link using the requests module as what we do in Section I-2. But we can illustrate the dynamic load using the lxml module. The code below tries to scrape the page link information using the lxml module. The scraper here has failed to extract the page links since the xpath() method returns an empty list. search_url = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/cases/search&quot; search_page = requests.get(search_url) search_html = html.fromstring(search_page.text) page_link = search_html.xpath(&#39;//*[@id=&quot;next&quot;]&#39;) print(page_link) ## [] Let us examine the page source code to see why. Here, we find that the &lt;div&gt; element with ID “pagination” is empty. If we scroll down the source code to the end, we can find that the display of the page links is coded in a JavaScript function displayResult(jsonresult) in the JavaScript section. This means that the web page has used JavaScript to load the page links and insert it at the position of the &lt;div&gt; element with ID “pagination” in the original HTML. We could see the revised HTML after running the JavaScript code in the Elements window. Figure I-3.2 Figure I-3.3 Now let us examine the second way of design – scroll down to the bottom of page – to load the new content. Let us look at another example web page, which is available at dynamic search load. This webpage is the same as the previous example webpage except it uses scrolling bar down instead of clicking page link to load the new content. The code below tries to scrap the information of the result table’s entries using the lxml module. The scraper here has failed to extract those information since the xpath() method returns an empty list. searchLoad_url = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/casesLoad/search&quot; searchLoad_page = requests.get(searchLoad_url) searchLoad_html = html.fromstring(searchLoad_page.text) entries_link = searchLoad_html.xpath(&#39;//*[@id=&quot;resultstable&quot;]/tbody/tr&#39;) print(page_link) ## [] Figure I-3.4 illustrates the result table part of code in the original HTML from the page source code. Figure I-3.5 highlights the same part of code in the revised HTML from the Elements window after it executes the JavaScript code. It is clear to see that there is no information under the tag name &lt;tbody&gt; in Figure I-3.4. This explains why the xpath() method returns an empty list. In Figure I-3.5, the webpage runs the JavaScript to insert the first chunk of the students’ information into the empty result table that has been created statically before running the JavaScript and then display it in the browser. Figure I-3.4 Figure I-3.5 More interestingly, once we open the Elements window of this example webpage, under the &lt;tbody&gt; tag, we find there are 15 table entries (with tag &lt;tr&gt;). We can therefore infer that a load has a total of 15 table entries. If we scroll down to the bottom of the webpage, we could see that 9 more entries are appended to the table. If we continue to scroll down to the bottom of the page, nothing is changed. This means that there exists a total of 24 table entries and the new load loads the next 9 new ones after the first batch. Your browser executes the JavaScript code to perform all these actions. If you look at the JavaScript section in the page source code, you will see that a JQuery method $(window).on(\"scroll\", function() defines the scrolling down to the bottom of page and triggers the load of the new content once it is satisfied. And another JQuery method $('#resultstable &gt; tbody:last-child').append(htmlContent) append the new entries to the result table. Here the screenshot of the relevant parts of the JavaScript code is not shown. "],
["finding-web-elements.html", "4 Finding Web Elements 4.1 Setting Up 4.2 Locating Web Elements 4.3 Demo 4.4 NoSuchElementException", " 4 Finding Web Elements Selenium works by automating browsers to load the website, retrieve the required data, and even take certain actions on the website. 4.1 Setting Up For the successful implementation of browser automation using Selenium, WebDriver needs to be set up. Let us go through the following steps to set up WebDriver for Google Chrome. Other major browsers have the similar steps. Install Selenium using a third-party installer such as pip to install it from the command line via this command: pip install selenium. Download the latest stable release of ChromeDriver from the website, appropriate to the platform. Unzip the downloaded chromedriver*.zip. An application file named chromedriver.exe should appear. It is recommended that we place the .exe file on the main folder containing the codes. Now that we have completed the setup for WebDriver and the Selenium Python library, let’s start to use this setup through the Python IDE. I have created this example page to run our scraper against. The following code simply loads the example page: from selenium import webdriver driver = webdriver.Chrome(&#39;C:\\\\Users\\\\JLiu\\\\Desktop\\\\Web_Tutorial\\\\chromedriver.exe&#39;) form_url = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/form/search&quot; driver.get(form_url) driver.close() driver.quit() To begin with, we import webdriver from selenium and set a path to chromedriver.exe. The path created will be required to load Google Chrome. Selenium does not contain its own web browser; it requires integration with third party browsers to run. The selenium.webdriver is used to implement various browsers, in this case, Google Chrome. The webdriver.Chrome() method is provided with the path of Chrome WebDriver so that it creates an object of the selenium.webdriver.chrome.webdriver.WebDriver class, called as “driver” in this case, which will now provide access to the various attributes and properties from webdriver. chromedriver.exe will be instantiated at this instance or upon creation of the driver object. The Terminal screen and an empty new window of Google Chrome will be loaded. The new window from Google Chrome is then provided with a URL using the get() function from webdriver. The get() method accepts the URL that is to be loaded on the browser. We provide the example website address as an argument to get(); the browser will start loading the URL, as shown in the following screenshot: As you can see in the above screenshot, a notice is displayed just below the address bar with the message “Chrome is being controlled by automated test software”. This message also confirms the successful execution of the selenium.webdriver activity, and it can be provided with further codes to act on or automate the page that has been loaded. Following successful execution of the code, it is recommended that you close and quit the driver to free up system resources. close() method terminates the loaded browser window. quit() method ends the WebDriver application. 4.2 Locating Web Elements After the new Google Chrome window is loaded with the URL provided, we then can find the elements that we need to act on. We first need to find the selector or locator information for those elements in interest. The easiest way to identify the information is to inspect pages using developer tools. Right-click to open the pop-up menu, then select the Inspect option. In the Element window, move the mouse over the DOM structure of the page until it reaches the desired element. We then need to find information such as what HTML tag is used for the element, the defined attribute, and the values for the attributes and the structure of the page. Next, we need to tell Selenium how to find a particular element or set of elements on a web page programmatically and simulate user actions on these elements. We just need to pass the information we identify in the first step to Selenium. Selenium provides various find_element_by methods to find an element based on its attribute/value criteria or selector value that we supply in script. If a matching element is found, an instance of WebElement is returned or the NoSuchElementException exception is thrown if Selenium is not able to find any element matching the search criteria. Selenium also provides various find_elements_by methods to locate multiple elements. These methods search and return a list of elements that match the supplied values. find_element_by_id() and find_elements_by_id() methods: They return an element or a set of elements that have matching ID attribute values. The find_elements_by_id() method returns all the elements that have the same ID attribute values. Let’s try finding the search button from the example website. Here is the HTML code for the search button with an ID attribute value defined as search. You can find this code if you inspect the site and reach this element in its DOM. &lt;input type=&quot;submit&quot; id=&quot;search&quot; value=&quot;Search&quot; name=&quot;q&quot; class=&quot;button&quot; /&gt; ``` Here is an example that uses the `find_element_by_id()` method to find the search button. We will pass the ID attribute&#39;s value, search, to the `find_element_by_id()` method: ```python search_button = driver.find_element_by_id(&quot;search&quot;) find_element_by_name() and find_elements_by_name() methods: They return element(s) that have matching name attribute value. The find_elements_by_name() method returns all the elements that have the same name attribute values. In the previous example, we can find the search button using its name attribute value instead of the ID attribute value in the following way: search_button = driver.find_element_by_name(&quot;q&quot;) find_element_by_class_name() and find_elements_by_class_name() methods: They return element(s) that have matching class attribute value. The find_elements_by_class_name() method returns all the elements that have the identical class name attribute values. In the previous example, we can find the search button using its class attribute value in following code: search_button = driver.find_element_by_class_name(&quot;button&quot;) find_element_by_tag_name() and find_elements_by_tag_name() methods: They find element(s) by their HTML tag name. The example page displays a search form which has several form fields to fill in. Each form field name is implemented using an &lt;th&gt; or table header cell tag inside a &lt;tr&gt; or table row tag as shown in the following HTML code: We will use the find_elements_by_tag_name() method to get all the form field names. In this example, we will first find the table body implemented as &lt;tbody&gt; using the find_element_by_tag_name() method and then get all the &lt;tr&gt; or table row elements by calling the find_elements_by_tag_name() method on the table body object. For each of the first 4 table rows, we then get its form field name using the &lt;th&gt; tag. table = driver.find_element_by_tag_name(&quot;tbody&quot;) entries = table.find_elements_by_tag_name(&quot;tr&quot;) for i in range(4): header = entries[i].find_element_by_tag_name(&quot;th&quot;).text print(header) find_element_by_xpath() and find_elements_by_xpath() methods: They return element(s) that are found by the specified XPath query. XPath is a query language used to search and locate nodes in an XML document. All the major web browsers support XPath. Selenium can leverage and use powerful XPath queries to find elements on a web page. One of the advantages of using XPath is when we can’t find a suitable ID, name, or class attribute value for the element. We can use XPath to either find the element in absolute terms or relative to an element that does have an ID or name attribute. We can also use defined attributes other than the ID, name, or class with XPath queries. We can also find elements with the help of a partial check on attribute values using XPath functions such as starts-with(), contains(), and ends-with(). For example, we want to get the second form field name “Grade”. This element is defined as an &lt;th&gt; tag, but does not have the ID, name, or class attributes defined. Also, we cannot use the find_element_by_tag_name() method as there are multiple &lt;tr&gt; and &lt;th&gt; tags defined on the page. In this case, we can use the find_element_by_xpath() method. To find the XPath of this element, we inspect the example site, in the Element window, move the mouse over its DOM structure and find the desired element. We then right-click the mouse and choose copy XPath from the pop-up menu. We obtain the XPath of this element as follows: //*[@id=&quot;table&quot;]/tbody/tr[2]/th This XPath says that the path to this element starts from the root and then finds the element with a unique id and then continue until it reaches the desired element. Please note that the index of the XPath always starts with 1 rather than o, unlike those of the built-in Python data structures. We then pass this XPath to the find_element_by_xpath() method as an argument: second_header = driver.find_element_by_xpath(&#39;//*[@id=&quot;table&quot;]/tbody/tr[2]/th&#39;).text We use the XPath method when it exists an element with a unique id on the path to the desired element. Otherwise, this method is not relaible. find_element_by_css_selector() and find_elements_by_css_selector() methods: They return element(s) that are found by the specified CSS selector. CSS is a style sheet language used by web designers to describe the look and feel of an HTML document. CSS is used to define various style classes that can be applied to elements for formatting. CSS selectors are used to find HTML elements based on their attributes such as ID, classes, types, attributes, or values and much more to apply the defined CSS rules. Similar to XPath, Selenium can leverage and use CSS selectors to find elements on a web page. In the previous example in which we want to get the search button on the example site, we can use the following selector, where the selector is defined as the element tag along with the class name. This will find an &lt;input&gt; element with the “btn-default” class name. We then test it by automating a click on the search button object we found and find if it starts the search successfully. search_button = driver.find_element_by_css_selector(&quot;input.btn-default&quot;) search_button.click() find_element_by_link_text() and find_elements_by_link_text() methods: They find link(s) using the text displayed for the link. The find_elements_by_link_text() method gets all the link elements that have matching link text. For example, we want to get the privacy policy link displayed on the example site. Here is the HTML code for the privacy policy link implemented as the &lt;a&gt; or anchor tag with text “privacy policy”: This is the &lt;a id=&quot;privacy_policy&quot; href=&quot;/tutorial/static/views/privacy.html&quot;&gt;privacy policy.&lt;/a&gt;&lt;br/&gt; ``` Let&#39;s create a test that locates the privacy policy link using its text and check whether it&#39;s displayed: ```python privacypolicy_link = driver.find_element_by_link_text(&quot;privacy policy.&quot;) privacypolicy_link.click() find_element_by_partial_link_text() and find_elements_by_partial_link_text() methods: They find link(s) using partial text. For example, on the example site, two links are displayed: one is the privacy policy link with “privacy policy” as text and the other is the term conditions policy link with “term conditions policy” as text. Let us use this method to find these links using the “policy” text and check whether we have two of these links available on the page: policy_links = driver.find_elements_by_partial_link_text(&quot;policy&quot;) print(len(policy_links)) 4.3 Demo This section will show you two cases to demonstrate the use of various find_elements_by methods. Most often we want to scrape data from tables or article texts. Demos therefore cover these two cases. 4.3.1 Scrape Tables Let us examine this example website. This page uses JavaScript to write a table to a &lt;div&gt; element of the page. If we were to scrape this page’s table using traditional methods, we’d only get the loading page, without actually getting the data that we want. Suppose that we want to scrape all cells of this table. The first thing we need to do is to locate the cells. If we inspect this page, we can see that the table is defined with a &lt;tbody&gt; tag inside a &lt;table&gt; tag. Each table row is defined with a &lt;tr&gt; tag and there are multiple table rows. The first table row is the table header row, each of its fields is defined with a &lt;th&gt; tag or a header cell tag. In each of the rest table rows, there are multiple data cells and each data cell is defined with a &lt;td&gt; tag. The information is used for us to locate those elements. Next, we have to find each cell using find_elements_by methods and get its data. There is no way to directly scrape the whole table. Given this, the logic naturally is to loop row by row, and in each row, loop cell by cell. So, we need to have a double looping in our script. Finally, we need to store the scraped data in a nice format, like a .csv file. We use the Python file operation methods to achieve this. The following program shows the way we do it: from selenium import webdriver import time driver = webdriver.Chrome(&#39;C:\\\\Users\\\\JLiu\\\\Desktop\\\\Web_Tutorial\\\\chromedriver.exe&#39;) table_url = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/default/dynamic&quot; driver.get(table_url) time.sleep(2) file = open(&#39;C:\\\\Users\\\\JLiu\\\\Desktop\\\\Web_Tutorial\\\\table.csv&#39;, &quot;w&quot;, encoding = &quot;utf-8-sig&quot;) table_body = driver.find_element_by_xpath(&#39;//*[@id=&quot;result&quot;]/table/tbody&#39;) entries = table_body.find_elements_by_tag_name(&#39;tr&#39;) headers = entries[0].find_elements_by_tag_name(&#39;th&#39;) table_header = &#39;&#39; for i in range(len(headers)): header = headers[i].text if i == len(headers) - 1: table_header = table_header + header + &quot;\\n&quot; else: table_header = table_header + header + &quot;,&quot; file.write(table_header) for i in range(1, len(entries)): cols = entries[i].find_elements_by_tag_name(&#39;td&#39;) table_row = &#39;&#39; for j in range(len(cols)): col = cols[j].text if j == len(cols) - 1: table_row = table_row + col + &quot;\\n&quot; else: table_row = table_row + col + &quot;,&quot; file.write(table_row) driver.close() file.close() 4.3.2 Scrape Texts Let us examine this example website. The article on this page has many subsections, each of which has multiple paragraphs and even bullet points. Suppose that we want to scrape the whole text of the article. One interesting way to do it is to scrape all the subsections separately first and then concatenate them altogether. The advantage of doing it in this way is obviously that we can also get each subsection text, which will greatly facilitate our later analysis, in particular the text analysis. Let us inspect this website. Let us move the mouse to the element of its DOM that defines the article content area. Under this &lt;div&gt; element, we can see that subsection headers have tag names all starting with “h”, paragraphs have a &lt;p&gt; tag name, and bullet points parts have a &lt;ul&gt; tag name. The elements with these tag names are all in parallel with each other, rather than embedded in a hierarchical structure. This design decides that we will not write a loop in our script to access them, for example, to access each paragraph under a subsection. Another point to note is that we use a Python dictionary to store each subsection text. For each key-value pair in this dictionary, the key stores the subsection title, and the value stores its paragraphs of text. So, we think this is a nice data structure to use for this case. The following program implements our strategy above to scrape the whole text of the article: from selenium import webdriver import time driver = webdriver.Chrome(&quot;C:\\\\Users\\\\JLiu\\\\Desktop\\\\Web_Tutorial\\\\chromedriver.exe&quot;) journalAddress = &quot;https://www.federalregister.gov/documents/2013/09/24/2013-21228/affirmative-action-and-nondiscrimination-obligations-of-contractors-and-subcontractors-regarding&quot; driver.get(journalAddress) time.sleep(2) articleObjects = driver.find_elements_by_xpath(&#39;//div[@id=&quot;fulltext_content_area&quot;]/*&#39;) print(len(articleObjects), &quot;\\n\\n&quot;) articleDictionary = dict() myKey = &quot;&quot; myValue_total = &quot;&quot; for i in range(len(articleObjects)): tagName = articleObjects[i].tag_name if tagName.startswith(&quot;h&quot;): if myKey: articleDictionary[myKey] = myValue_total myKey = &quot;&quot; myValue_total = &quot;&quot; myKey = articleObjects[i].get_attribute(&quot;innerText&quot;) if tagName.startswith(&quot;p&quot;): myValue = articleObjects[i].get_attribute(&quot;innerText&quot;) myValue_total = myValue_total + myValue if tagName.startswith(&quot;ul&quot;): myBullets = articleObjects[i].find_elements_by_tag_name(&#39;li&#39;) for j in range(len(myBullets)): myBullet = myBullets[j].get_attribute(&quot;innerText&quot;) myValue_total = myValue_total + myBullet driver.close() article = &#39;&#39; for key, value in articleDictionary.items(): article = article + key + &#39;\\n\\n&#39; + value + &#39;\\n\\n***************\\n\\n&#39; print(article) 4.4 NoSuchElementException The reason for NoSuchElementException can be either of the following: The Locator Strategy you have adopted doesn’t identifies any element in the HTML DOM. The Locator Strategy you have adopted is unable to identify the element as it is not within the browser’s Viewport. The Locator Strategy you have adopted identifies the element but is invisible due to presence of the attribute style=“display: none;”. The Locator Strategy you have adopted doesn’t uniquely identifies the desired element in the HTML DOM and currently finds some other hidden / invisible element. The WebElement you are trying to locate is within an &lt;iframe&gt; tag. The WebDriver instance is looking out for the WebElement even before the element is present/visibile within the HTML DOM. The solution to address NoSuchElementException can be either of the following: When the element you locate does not exist in the DOM, use try-except event handler to avoid the termination of the program. This solution is to address the inconsistency in the DOM among the seemingly same pages. See the example code as follows: from selenium.common.exceptions import NoSuchElementException try: elem = driver.find_element_by_xpath(&quot;element_xpath&quot;) elem.click() except NoSuchElementException: pass When the page loads for some reason you are taken to the bottom of the page, but the element you need to scrape is on the top of the page and thus is out of view, you can locate the element in the DOM first, then use execute_script() method to scroll the element in to view as follows: elem = driver.find_element_by_xpath(&quot;element_xpath&quot;) driver.execute_script(&quot;arguments[0].scrollIntoView();&quot;, elem) In case the element is having the attribute style=“display: none;”, remove the attribute through execute_script() method as follows: elem = driver.find_element_by_xpath(&quot;element_xpath&quot;) driver.execute_script(&quot;arguments[0].removeAttribute(&#39;style&#39;)&quot;, elem) elem.send_keys(&quot;text_to_send&quot;) Adopt a Locator Strategy which uniquely identifies the desired WebElement. The preferable method is find_element(s)_by_id() method since the id attribute uniquely identifies a web element. To check if the element is within an &lt;iframe&gt;, traverse up the HTML to locate the respective &lt;iframe&gt; tag and use the switch_to() to shift to the desired iframe through either of the following methods: driver.switch_to.frame(&quot;iframe_name&quot;) driver.switch_to.frame(&quot;iframe_id&quot;) driver.switch_to.frame(1) // 1 represents frame index We can switch back to the main frame by using one of the following methods: driver.switch_to.default_content() driver.switch_to.parent_frame() A better way to switch frames will be to induce WebDriverWait() for the availability of the intended frame with expected_conditions set to frame_to_be_available_and_switch_to_it as either of the follows: WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it(By.ID,&quot;id_of_iframe&quot;)) // through Frame ID WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it(By.NAME,&quot;name_of_iframe&quot;)) // through Frame name WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it(By.XPATH,&quot;xpath_of_iframe&quot;)) // through Frame XPath WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it(By.CSS_SELECTOR,&quot;css_of_iframe&quot;)) // through Frame CSS If the element is not present/visible in the HTML DOM immediately, induce WebDriverWait with expected_conditions set to proper method as follows: To wait for presence_of_element_located: element = WebDriverWait(driver, 20).until(expected_conditions.presence_of_element_located((By.XPATH, &quot;element_xpath&#39;]&quot;))) To wait for visibility_of_element_located: element = WebDriverWait(driver, 20).until(expected_conditions.visibility_of_element_located((By.CSS_SELECTOR, &quot;element_css&quot;))) To wait for element_to_be_clickable: element = WebDriverWait(driver, 20).until(expected_conditions.element_to_be_clickable((By.LINK_TEXT, &quot;element_link_text&quot;))) "],
["handle-web-forms.html", "5 Handle Web Forms 5.1 Input Box 5.2 Check Box 5.3 Radio Button 5.4 Link 5.5 Dropdown 5.6 Buttons 5.7 Demos 5.8 ElementNotInteractableException", " 5 Handle Web Forms You must have noticed the ubiquity of web forms while surfing the internet. A web form comprises web elements such as input box, check box, radio button, link, drop down menu and submit button to collect user data. To process web forms, we would need to first find these web elements and then take subsequent actions on them like selecting a value or entering some text. Selenium has an API that helps us to do that. Since we have covered how to find web element(s) using Selenium selectors in the previous section, this section focuses on the crux of accessing form in Selenium: performing actions on and interacting with the forms using Selenium. Let us see how different actions can be performed on each type of web fields that may be involved in your web form. We use this example login form website to illustrate most of the examples used in this section. Below is a snapshot of how this website uses forms: 5.1 Input Box Input box is the text box that displays user input. To handle any input box, we must be able to enter information, clear information or get information from the box. To enter text into a textbox we can use the send_keys() method which would input the user required text from our automation script. The following code enters the student name starting with “A” into a text box whose ID is “search_name”: driver.find_element_by_id(&#39;search_name&#39;).send_keys(&quot;A&quot;) To clear a pre-entered text or the text that you last entered can be wiped clean with the clear() method. For example: driver.find_element_by_id(&#39;search_name&#39;).clear() To fetch the text that is written in a text box in case we need to validate some existing or entered text, use the get_attribute() method: nameText = driver.find_element_by_id(&#39;search_name&#39;).get_attribute(&quot;value&quot;) print(nameText) 5.2 Check Box Check box is a small box that enables us to check or uncheck it. To select or check a value we use the click() method. It simply changes the state from unchecked to checked and vice-versa. For example, the code below clicks on the “Accept Privacy Policy” checkbox: driver.find_element_by_id(&#39;privacypolicy&#39;).click() It is not always straightforward anyway. Because we may need to select checkbox only when it is not selected already; Likewise, checkbox deselection, we want deselect checkbox only when it is selected. To select, if we perform click operation on already selected checkbox, then the checkbox will be deselected; Something we do not want to happen. So, we need to validate whether the checkbox is selected or not. To get the state we can use two mechanisms: is_selected() OR get_attribute(“checked”). For example: privacy_boolean = driver.find_element_by_id(&#39;privacypolicy&#39;).is_selected() print(privacy_boolean) print(type(privacy_boolean)) This will return a boolean value. This means if the checkbox is checked we would get a True else we will get False. OR: privacy_other = driver.find_element_by_id(&#39;privacypolicy&#39;).get_attribute(&quot;checked&quot;) print(privacy_other) print(type(privacy_other)) This will return True if the checkbox is selected. But will return NoneType if checkbox is not selected. The following code shows how to deselect a checkbox only when it is selected: driver.find_element_by_id(&#39;privacypolicy&#39;).click() 5.3 Radio Button A radio button is a circular element on the screen that we can select. It is similar to a checkbox only difference being if we are given multiple radio buttons, we can select just one, while in the case of multiple checkboxes, we can opt multiple of them. The actions performed on the radio button are similar to those on a checkbox and we can use the same methods as above to select a radio button or validate its status of selection: click() and is_selected() / get_attribute(“checked”). The code below provides an example: driver.find_element_by_id(&#39;p5&#39;).click() pageSize_5_boolean = driver.find_element_by_id(&#39;p5&#39;).is_selected() print(pageSize_5_boolean) print(type(pageSize_5_boolean)) pageSize_5_other = driver.find_element_by_id(&#39;p5&#39;).get_attribute(&quot;checked&quot;) print(pageSize_5_other) print(type(pageSize_5_other)) Radio button does not support deselection. To deselect a radio button, one needs to select any other radio button in that group. If you select the same radio button trying to deselect it, you will get the same selection as before; nothing will change. The following code shows how to deselect a radio button only when it is selected: driver.find_element_by_id(&#39;p10&#39;).click() 5.4 Link A link redirects us to a new web page or a new window pop-up or a similar thing. We can use two mechanisms to navigate to a new screen or a new pop up or a new form: we can either do a click action on a link element we find, or get the new URL from the link element we find and then navigate to it. Here is an example of operating the link embedded in the example webpage using the first mechanism: driver.find_element_by_id(&quot;privacy_policy&quot;).click() Links are generally embedded in the link element we find with a &lt;a&gt; tag name as the “href” property. Instead of directly clicking on the link, we can use the get_attribute() method. Here is the same example using the second mechanism: privacy_object = driver.find_element_by_id(&quot;privacy_policy&quot;) privacy_link = privacy_object.get_attribute(&quot;href&quot;) driver.get(privacy_link) 5.5 Dropdown Dropdown is a list which has an arrow at the rightmost end to expand and show the values. It provides a list of options to the user thereby giving access to one or multiple values as per the requirement. To work with a dropdown, first you need to select or find the main element group and then go inside further and select the sub-element that you want to select for. Selenium Python API provides the Select class, which allows you to select the element of your choice. Note that the Select class only works with tags which have &lt;select&gt; tags. We can select a sub-element using (1) the Index of Dropdown, (2) the Value of Dropdown, or (3) the Text of Dropdown. If the dropdown has an “index” attribute, then we can use that index to select a particular option. You need to be careful while using this way, because it is not un-common to have the index start at 0. We can use the select_by_index() method to select an option using the “index” attribute. For example, we want to select the 5th grade students: grade_dropdown = Select(driver.find_element_by_id(&quot;search_grade&quot;)) grade_dropdown.select_by_index(6) If the html mark-up defines an &lt;option&gt; tag, then you can use the value matching the argument. Suppose the html for dropdown is like this: &lt;td&gt; &lt;select id=&quot;search_grade&quot;&gt; &lt;option selected&gt;(no value)&lt;/option&gt; &lt;option value=&quot;K&quot;&gt;K&lt;/option&gt; &lt;option value=&quot;1&quot;&gt;1&lt;/option&gt; &lt;option value=&quot;2&quot;&gt;2&lt;/option&gt; &lt;option value=&quot;3&quot;&gt;3&lt;/option&gt; &lt;option value=&quot;4&quot;&gt;4&lt;/option&gt; &lt;option value=&quot;5&quot;&gt;5&lt;/option&gt; &lt;/select&gt; &lt;/td&gt; We can use the select_by_value() method to select an option using the “value” attribute. grade_dropdown.select_by_value(&quot;5&quot;) Probably the easiest way of doing it is to select an element using text of dropdown. You have to match the text which is displayed in the drop down using the select_by_visible_text() method. For example, grade_dropdown.select_by_visible_text(&quot;5&quot;) In a similar way we can deselect any selected value from the dropdown using any of the below options: (1) deselect_by_index(), (2) deselect_by_value(), and (3) deselect_by_visible_text(). These methods can be used only when you make multiple selections. The deselect_all() method clears all the selected options. This is also only applicable in case of multiple selections. If you try to use this in case of single selection, this will throw a NotImplementedError exception. There are times while performing Selenium automation of our web app, where we need to validate the options coming in our dropdown list. The Select class provides property methods that allow us to do it. The first two property methods are applicable wherein we can select multiple options. .all_selected_options - Get the list of all the selected options. .first_selected_option - Return the first option that has been selected from the dropdown and unlike the above method it would return a single web element and not a list. .is_multiple - Return True if the dropdown allows multi-selection and return NoneType otherwise. .options - Get a list of all available options in a dropdown. 5.6 Buttons Buttons are simply used to submit whatever information we have filled in our forms to the server. This can be done through click actions on the button, mostly using the click() method: driver.find_element_by_id(&quot;search&quot;).click() 5.7 Demos I will first show you a simple program that handles all the types of form elements we have covered so far altogether using our example site. Second, using our example site, I will show you an advanced program that handles the situation where you will need to access this form not once but many times sequentially. 5.7.1 Fill in the Form Just Once Suppose that we want to search all the 5th grade students whose names start with “A” and page size set at 5. The program below shows you how to fill in the form and submit it to the server: from selenium import webdriver from selenium.webdriver.support.select import Select driver = webdriver.Chrome(&#39;C:\\\\Users\\\\JLiu\\\\Desktop\\\\Web_Tutorial\\\\chromedriver.exe&#39;) form_url = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/form/search&quot; driver.get(form_url) driver.find_element_by_id(&#39;search_name&#39;).send_keys(&quot;A&quot;) Select(driver.find_element_by_id(&quot;search_grade&quot;)).select_by_visible_text(&quot;5&quot;) driver.find_element_by_id(&#39;p5&#39;).click() driver.find_element_by_id(&quot;privacypolicy&quot;).click() driver.find_element_by_id(&quot;termsconditions&quot;).click() driver.find_element_by_id(&quot;search&quot;).click() 5.7.2 Fill in the Form Many Times Now suppose that we want to search all the students. This will require us to fill in the form many times, each time with changing input. We have to first play with the website to see if the form webpage will be refreshed every time you access to it. This will determine how we are going to code it. If the form page is refreshed every time you access to the form, then you have to refill in all the form fields each time even if most of those fields are repetitive inputs. The program below shows an example of this case: from selenium import webdriver import time import string import pandas as pd driver = webdriver.Chrome(&#39;C:\\\\Users\\\\JLiu\\\\Desktop\\\\Web_Tutorial\\\\chromedriver.exe&#39;) searchAddress = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/form/search&quot; driver.get(searchAddress) time.sleep(2) students = list() for letter in string.ascii_uppercase: for grade in range(2,8): driver.find_element_by_xpath(&#39;//*[@id=&quot;search_name&quot;]&#39;).send_keys(letter) driver.find_element_by_xpath(&#39;//*[@id=&quot;search_grade&quot;]/option[{}]&#39;.format(grade)).click() driver.find_element_by_id(&#39;p5&#39;).click() driver.find_element_by_id(&quot;privacypolicy&quot;).click() driver.find_element_by_id(&quot;termsconditions&quot;).click() driver.find_element_by_xpath(&#39;//*[@id=&quot;search&quot;]&#39;).click() time.sleep(5) try: while True: table = driver.find_element_by_xpath(&#39;//*[@id=&quot;results&quot;]/table&#39;) entries = table.find_elements_by_tag_name(&quot;tr&quot;) for i in range(1, len(entries)): student_dict = dict() cols = entries[i].find_elements_by_tag_name(&quot;td&quot;) student_dict[&quot;name&quot;] = cols[0].text student_dict[&quot;grade&quot;] = cols[1].text student_dict[&quot;gpa&quot;] = cols[2].text students.append(student_dict) try: driver.find_element_by_xpath(&#39;//*[@id=&quot;next&quot;]&#39;).click() time.sleep(2) except: break driver.get(searchAddress) time.sleep(2) except: print(&quot;No results for letter {} at grade {}&quot;.format(letter, grade - 2)) driver.get(searchAddress) time.sleep(2) students_df = pd.DataFrame.from_records(students) print(students_df) driver.close() Line 39 and 43 of this program (driver.get(searchAddress)) tells us that the form will be refreshed whenever we finish the current search. It could be either when we find out that there is no result for this search or when we have scraped the results for this search. Since the form is refreshed every time for a new search, we need to refill the form every time when a new search starts. This is why lines 15 to 19 of the code which execute the form filling actions are put in the most inner layer of loop. Another question is that when the form is refreshed for a new search, how could we know where the search has gone so far at this point? We must know this so that we can know what we will input for the new search. We control the moving of rounds of search by using the indexing of a list. We put all the options for a form field into a created list with the exact order of the list elements as that being displayed in the form field. When we loop this list, we can control where the search has run through. In this example, line 13 and 14 play this role. They control the indexing of the name list and the grade list. The name field and the grade field are the only two fields whose input values will change for a new search. This is why we need to create a list for them and then loop over it rather than doing the same thing for all the other form fields. Another scenario is that the form page is not refreshed for a new search. In this case, then we do not need to refill in the form fields that will not change their values in a new search. Below is the program that performs the same task – search all students – but in the situation that the form is not refreshed: from selenium import webdriver import time import string import pandas as pd driver = webdriver.Chrome(&#39;C:\\\\Users\\\\JLiu\\\\Desktop\\\\Web_Tutorial\\\\chromedriver.exe&#39;) searchAddress = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/form/search&quot; driver.get(searchAddress) time.sleep(2) students = list() driver.find_element_by_id(&#39;p5&#39;).click() driver.find_element_by_id(&quot;privacypolicy&quot;).click() driver.find_element_by_id(&quot;termsconditions&quot;).click() for letter in string.ascii_uppercase: driver.find_element_by_xpath(&#39;//*[@id=&quot;search_name&quot;]&#39;).clear() driver.find_element_by_xpath(&#39;//*[@id=&quot;search_name&quot;]&#39;).send_keys(letter) for grade in range(2,8): driver.find_element_by_xpath(&#39;//*[@id=&quot;search_grade&quot;]/option[{}]&#39;.format(grade)).click() driver.find_element_by_xpath(&#39;//*[@id=&quot;search&quot;]&#39;).click() time.sleep(5) try: while True: table = driver.find_element_by_xpath(&#39;//*[@id=&quot;results&quot;]/table&#39;) entries = table.find_elements_by_tag_name(&quot;tr&quot;) for i in range(1, len(entries)): student_dict = dict() cols = entries[i].find_elements_by_tag_name(&quot;td&quot;) student_dict[&quot;name&quot;] = cols[0].text student_dict[&quot;grade&quot;] = cols[1].text student_dict[&quot;gpa&quot;] = cols[2].text students.append(student_dict) try: driver.find_element_by_xpath(&#39;//*[@id=&quot;next&quot;]&#39;).click() time.sleep(2) except: break except: print(&quot;No results for letter {} at grade {}&quot;.format(letter, grade - 2)) students_df = pd.DataFrame.from_records(students) print(students_df) driver.close() We cannot find the lines of code in the program that refresh the form page. We put the lines of code that fill in the constant form fields outside of the loop so that those constant fields will not be refilled in every time for a new search. The change of the positions of lines 13 – 15 and line 19 of the code comparing the program in the first scenario reflects this idea. 5.8 ElementNotInteractableException In some cases when the element is not interactable, actions on it as introduced in the above sections do not work and you are likely to encounter an ElementNotInteractableException. ElementNotInteractableException is caused when an element is found, but you cannot interact with it. For instance, you may not be able to click or send keys. There could be several reasons for this: The element is not visible / not displayed. The element is off screen. The element is behind another element or hidden. Some other action needs to be performed by the user first to enable it. There are strategies that may work to make it interactable, depending on the circumstance. 5.8.1 Wait Until the Element is Clickable If the element has not been fully downloaded yet, we can wait until the element is visible / clickable. Look at the following example in which we want to get the profile for the 5th grade student named as “Adams”. from selenium import webdriver from selenium.webdriver.support.select import Select driver = webdriver.Chrome(&#39;C:\\\\Users\\\\JLiu\\\\Desktop\\\\Web_Tutorial\\\\chromedriver.exe&#39;) form_url = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/form/search&quot; driver.get(form_url) driver.find_element_by_id(&#39;search_name&#39;).send_keys(&quot;A&quot;) Select(driver.find_element_by_id(&quot;search_grade&quot;)).select_by_visible_text(&quot;5&quot;) driver.find_element_by_id(&#39;p5&#39;).click() driver.find_element_by_id(&quot;privacypolicy&quot;).click() driver.find_element_by_id(&quot;termsconditions&quot;).click() driver.find_element_by_id(&quot;search&quot;).click() table = driver.find_element_by_xpath(&#39;//*[@id=&quot;results&quot;]/table&#39;) entries = table.find_elements_by_tag_name(&quot;tr&quot;) fields = entries[1].find_elements_by_tag_name(&#39;td&#39;) fields[3].find_element_by_tag_name(&quot;a&quot;).click() This code produces an error message that “no such element: Unable to locate element” because the result table has not been fully downloaded yet. Selenium WebDriver provides two types of waits to handle it – explicit and implicit wait. The time.sleep() method is an explicit wait to set the condition to be an exact time period to wait, as the code below shows: from selenium import webdriver from selenium.webdriver.support.select import Select import time driver = webdriver.Chrome(&#39;C:\\\\Users\\\\JLiu\\\\Desktop\\\\Web_Tutorial\\\\chromedriver.exe&#39;) form_url = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/form/search&quot; driver.get(form_url) driver.find_element_by_id(&#39;search_name&#39;).send_keys(&quot;A&quot;) Select(driver.find_element_by_id(&quot;search_grade&quot;)).select_by_visible_text(&quot;5&quot;) driver.find_element_by_id(&#39;p5&#39;).click() driver.find_element_by_id(&quot;privacypolicy&quot;).click() driver.find_element_by_id(&quot;termsconditions&quot;).click() driver.find_element_by_id(&quot;search&quot;).click() time.sleep(3) table = driver.find_element_by_xpath(&#39;//*[@id=&quot;results&quot;]/table&#39;) entries = table.find_elements_by_tag_name(&quot;tr&quot;) fields = entries[1].find_elements_by_tag_name(&#39;td&#39;) fields[3].find_element_by_tag_name(&quot;a&quot;).click() As discussed in the previous chapter, a more efficient solution would be to make WebDriver wait only as long as required. This is also an explicit wait but more efficient than time.sleep(). The code below uses the presence of the resulting table element with id “resulttable” to declare that the page has been fully loaded: from selenium import webdriver from selenium.webdriver.support.select import Select from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC driver = webdriver.Chrome(&#39;C:\\\\Users\\\\JLiu\\\\Desktop\\\\Web_Tutorial\\\\chromedriver.exe&#39;) form_url = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/form/search&quot; driver.get(form_url) driver.find_element_by_id(&#39;search_name&#39;).send_keys(&quot;A&quot;) Select(driver.find_element_by_id(&quot;search_grade&quot;)).select_by_visible_text(&quot;5&quot;) driver.find_element_by_id(&#39;p5&#39;).click() driver.find_element_by_id(&quot;privacypolicy&quot;).click() driver.find_element_by_id(&quot;termsconditions&quot;).click() driver.find_element_by_id(&quot;search&quot;).click() table = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, &quot;resulttable&quot;))) entries = table.find_elements_by_tag_name(&quot;tr&quot;) fields = entries[1].find_elements_by_tag_name(&#39;td&#39;) fields[3].find_element_by_tag_name(&quot;a&quot;).click() The final solution is to use an implicit wait, which tells WebDriver to poll the DOM for a certain amount of time when trying to find any element (or elements) not immediately available. The default setting is 0. Once set, the implicit wait is set for the life of the WebDriver object. from selenium import webdriver from selenium.webdriver.support.select import Select driver = webdriver.Chrome(&#39;C:\\\\Users\\\\JLiu\\\\Desktop\\\\Web_Tutorial\\\\chromedriver.exe&#39;) driver.implicitly_wait(10) form_url = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/form/search&quot; driver.get(form_url) driver.find_element_by_id(&#39;search_name&#39;).send_keys(&quot;A&quot;) Select(driver.find_element_by_id(&quot;search_grade&quot;)).select_by_visible_text(&quot;5&quot;) driver.find_element_by_id(&#39;p5&#39;).click() driver.find_element_by_id(&quot;privacypolicy&quot;).click() driver.find_element_by_id(&quot;termsconditions&quot;).click() driver.find_element_by_id(&quot;search&quot;).click() table = driver.find_element_by_xpath(&#39;//*[@id=&quot;results&quot;]/table&#39;) entries = table.find_elements_by_tag_name(&quot;tr&quot;) fields = entries[1].find_elements_by_tag_name(&#39;td&#39;) fields[3].find_element_by_tag_name(&quot;a&quot;).click() 5.8.2 Scroll Until the Element is On-Screen An element is on screen means that this element is embedded in the DOM structure of the web page. You can find its DOM structure when inspecting a web page. If the element you act upon is not in the DOM structure, then you will get some kind of error message due to this reason. Let us look at this example website. Suppose that we search all the 5th grade students whose name starts with “A”. If we inspect it, we can find that there are 15 table entries included under the &lt;tbody&gt; tag in its DOM structure, as shown in the following figure. Suppose that we want to get the profile for student “Aiden” – the 15th record of the page. It will work fine. But if we want to get the profile for the 16th student of the page, we will get an error message saying that “list index out of range”. This is because that record is off-screen. from selenium import webdriver from selenium.webdriver.support.select import Select import time driver = webdriver.Chrome(&#39;C:\\\\Users\\\\JLiu\\\\Desktop\\\\Web_Tutorial\\\\chromedriver.exe&#39;) form_url = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/casesLoad/search&quot; driver.get(form_url) driver.find_element_by_id(&#39;search_name&#39;).send_keys(&quot;A&quot;) Select(driver.find_element_by_id(&quot;search_grade&quot;)).select_by_visible_text(&quot;5&quot;) driver.find_element_by_id(&quot;search&quot;).click() time.sleep(5) table = driver.find_element_by_xpath(&#39;//*[@id=&quot;results&quot;]/table&#39;) table_body = table.find_element_by_tag_name(&quot;tbody&quot;) entries = table_body.find_elements_by_tag_name(&quot;tr&quot;) fields = entries[15].find_elements_by_tag_name(&#39;td&#39;) fields[3].find_element_by_tag_name(&#39;a&#39;).click() The solution is that we can scroll down the page so that the off-screen records can be added into its DOM structure. You can use the following code to scroll down to a certain height of the page, where “5000” is the height, or use it to scroll down to the bottom of the page: from selenium import webdriver from selenium.webdriver.support.select import Select import time driver = webdriver.Chrome(&#39;C:\\\\Users\\\\JLiu\\\\Desktop\\\\Web_Tutorial\\\\chromedriver.exe&#39;) form_url = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/casesLoad/search&quot; driver.get(form_url) driver.find_element_by_id(&#39;search_name&#39;).send_keys(&quot;A&quot;) Select(driver.find_element_by_id(&quot;search_grade&quot;)).select_by_visible_text(&quot;5&quot;) driver.find_element_by_id(&quot;search&quot;).click() time.sleep(5) # scroll to a certain height driver.execute_script(&quot;window.scrollTo(0, 5000)&quot;) # scroll to the bottom #driver.execute_script(&quot;window.scrollTo(0, document.body.scrollHeight);&quot;) time.sleep(5) table = driver.find_element_by_xpath(&#39;//*[@id=&quot;results&quot;]/table&#39;) table_body = table.find_element_by_tag_name(&quot;tbody&quot;) entries = table_body.find_elements_by_tag_name(&quot;tr&quot;) fields = entries[15].find_elements_by_tag_name(&#39;td&#39;) fields[3].find_element_by_tag_name(&#39;a&#39;).click() 5.8.3 Execute JavaScript to Interact Directly with the DOM On this example website, we can see that the form submit button is hidden. If we still use the button click() method, we will get an error message saying that “element not interactable”. In this case, you can opt to execute JavaScript that directly interacts with the DOM: from selenium import webdriver from selenium.webdriver.support.select import Select driver = webdriver.Chrome(&#39;C:\\\\Users\\\\JLiu\\\\Desktop\\\\Web_Tutorial\\\\chromedriver.exe&#39;) driver.implicitly_wait(10) form_url = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/formhidden/search&quot; driver.get(form_url) driver.find_element_by_id(&#39;search_name&#39;).send_keys(&quot;A&quot;) Select(driver.find_element_by_id(&quot;search_grade&quot;)).select_by_visible_text(&quot;5&quot;) driver.find_element_by_id(&#39;p5&#39;).click() driver.find_element_by_id(&quot;privacypolicy&quot;).click() driver.find_element_by_id(&quot;termsconditions&quot;).click() driver.execute_script(f&#39;document.getElementById(&quot;search&quot;).click();&#39;) table = driver.find_element_by_xpath(&#39;//*[@id=&quot;results&quot;]/table&#39;) entries = table.find_elements_by_tag_name(&quot;tr&quot;) fields = entries[1].find_elements_by_tag_name(&#39;td&#39;) fields[3].find_element_by_tag_name(&#39;a&#39;).click() 5.8.4 Perform Whatever Other Action is Necessary Let us again search for all the 5th grade students whose name starts with “A” on this example website. If you move your mouse over a student name, you will see a hover box showing up above the name. Suppose that we want to scrape the data on the hover box. The hover box is not actionable unless we mover the mouse to a student name to enable it. Once we do that, if you inspect the webpage, you will see the hover box element has been added to its DOM structure. So, we can scrape the data on the hover box from there. The following is the code example that scrapes the content on the hover box of student “Adams”: from selenium import webdriver from selenium.webdriver.support.select import Select from selenium.webdriver.common.action_chains import ActionChains driver = webdriver.Chrome(&#39;C:\\\\Users\\\\JLiu\\\\Desktop\\\\Web_Tutorial\\\\chromedriver.exe&#39;) driver.implicitly_wait(10) form_url = &quot;https://iqssdss2020.pythonanywhere.com/tutorial/form/search&quot; driver.get(form_url) driver.find_element_by_id(&#39;search_name&#39;).send_keys(&quot;A&quot;) Select(driver.find_element_by_id(&quot;search_grade&quot;)).select_by_visible_text(&quot;5&quot;) driver.find_element_by_id(&#39;p5&#39;).click() driver.find_element_by_id(&quot;privacypolicy&quot;).click() driver.find_element_by_id(&quot;termsconditions&quot;).click() driver.find_element_by_id(&quot;search&quot;).click() table = driver.find_element_by_xpath(&#39;//*[@id=&quot;results&quot;]/table&#39;) entries = table.find_elements_by_tag_name(&quot;tr&quot;) fields = entries[1].find_elements_by_tag_name(&#39;td&#39;) name_tag = fields[0].find_element_by_tag_name(&quot;span&quot;) hov = ActionChains(driver).move_to_element(name_tag) hov.perform() hov_id = name_tag.get_attribute(&quot;aria-describedby&quot;) print(hov_id) hov_text = driver.find_element_by_id(hov_id).text print(hov_text) We first need to import ActionChains class in Selenium WebDriver. We create an ActionChains object by passing driver object. We then find the student name “Adams” object in the page and move cursor on this object using the method move_to_element(). Method perform() is used to execute the actions that we have built on the ActionChains object. In this case, this action makes the hover box appear above the student name. Once this is done, the hover box element is added to the DOM structure of the page. By inspecting this new addition in the DOM structure, we can find the ID of the hover box through attribute “aria-describedby” and therefore scrape the content of the hover box object associated with that ID. "]
]
